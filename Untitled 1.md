1) 상반기 진행한 업무:

   - Diffusion 및 채색 관련 논문 및 모델 위키 정리

   - 채색 실험

       a) Rectified flow 기반 채색

           현재 공개된 몇개의 모델을 기반으로 rectified flow를 학습하는 훈련을 진행하여 실험한 결과로 (i) 전반적으로 배경, 인물 구별 없이 색깔이 나타나거나, (ii) 얼굴 부분의 살색 정도만 어느정도 칠해지는 결과를 얻음.

       b) Text prompt 기반 채색

           위의 실패로 DreamBooth+LoRA Finetuning을 학습하여 Text prompt로 Lineart를 채색하는 실험을 진행

           - 아주 디테일한 부분은 제외하고 1024x1024 수준의 해상도에서는 어느 정도 학습이 되는 것을 확인

           - Text prompt 변경으로 Finetuning 할때 입지 않았던 옷의 색깔 editing이 어느정도 가능함을 확인

           - 서로 다른 캐릭터(노란머리 남자, 빨간머리 여자)에 대해 각각의 LoRA Finetuning Weight를 동시에 적용하여 Text-to-Image 생성을 했을 때, 하나의 캐릭터를 생성하는 것 보다는 불안정하긴 하지만 어느정도 구별하여 채색이 되는 것을 확인

   - 밑색 실험

       채색 과정이 러프하게 보면 선화 -> 밑색 -> 채색 으로 이루어 지는데 (밑색 -> 채색) 과정에서 작가나 제작자의 의도에 따라 수정가능성(변화가능성)이 큰 것에 비해 (선화 -> 밑색) 과정은 캐릭터 밑색팔레트만 확정되면 수정가능성이 적은 것으로 판단하여 밑색 실험 시작

           a) Animation 채색에 사용되는 PaintBucket Colorization로 어느 정도의 결과 확인

               - Closed Lineart인지에 따라 결과의 품질이 일정하지 않음 / Reference image에 따라 결과의 품질이 결정됨

           b) Image Restoration 관점으로 SDE 식을 세우고 이를 바탕으로 Diffusion 모델을 학습하는 방법 분석 및 진행중

           c) Segmentation 기반으로 머리, 피부, 옷 등을 구별하여 색칠하는 모델 분석 및 실험 진행중

               - Segmentation이 잘 되기만 한다면 가장 응용가능성이 높을 것으로 생각되는 방법

2) 과제의 방향성:

   - 일단 겉으로 보기에는 Reference 이미지와 유사한 느낌으로 채색이 되는 것을 상반기에 확인하였음.

   - 그러나 디테일하게 보면 Lineart의 Condition이 완전 100%유지되지는 않으므로(ControlNet의 한계), 이를 Woody가 진행중인 super resolution 방법 등으로 해결해야 할듯

   - 혹은, 뭔가 ControlNet을 대체하는 다른 방법을 찾거나 ControlNet 모델을 학습하는 등 다른 접근을 할 수도 있을 것 같음.

   - 또한 아주 디테일한 느낌의 채색이 되려면(채색 색깔의 느낌) 일단 밑색 Task가 완성된다면 이를 Oskar가 진행하는 작업의 Initial condition으로 사용하면 더 정확한 채색 느낌을 구현할 수 있을 것으로 보임.

3) 지원요청사항:

   - 지원요청사항이라기 보다는 현재 일단 "자동채색"을 목표로 작업을 수행중이긴 한데, 이를 사용할 작가나 작업자 입장에서 웹툰 채색 과정 중에 구현되었으면 하는 "구체적인 세부 작업"들이 있으면 이것이 무엇인지 저희 팀에게 전달되면 더 좋을 것 같음

   - 작가나 작업자가 그들의 작업중에 "창의성"을 요하는 일이 아니라 뭔가 노가다스러운 일인데 시간을 많이 잡아먹는 그런 작업프로세스가 있다면, 그 과정이 구체적으로 어느 과정이고 어떠한 작업인지 알 수 있다면 조금 더 모델을 개발하는 과정에서 그 pain point에 맞춰 개발을 진행할 수 있지 않을까 싶음