{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obsidian Notes","text":"<p>Publish your public notes with MkDocs</p>"},{"location":"#hello-world","title":"Hello World!","text":"<p>The <code>index.md</code> in the <code>/docs</code> folder is the homepage you see here.</p> <p>The folders in <code>/docs</code> appear as the main sections on the navigation bar.</p> <p>The notes appear as pages within these sections. For example, Note 1 in <code>Topic 1</code></p>"},{"location":"Features/LaTeX%20Math%20Support/","title":"LaTeX Math Support","text":"<p>LaTeX math is supported using MathJax.</p> <p>Inline math looks like \\(f(x) = x^2\\). The input for this is <code>$f(x) = x^2$</code>. Use <code>$...$</code>.</p> <p>For a block of math, use <code>$$...$$</code> on separate lines</p> <pre><code>$$\nF(x) = \\int^a_b \\frac{1}{2}x^4\n$$\n</code></pre> <p>gives </p> \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]"},{"location":"Features/Mermaid%20Diagrams/","title":"Mermaid diagrams","text":"<p>Here's the example from MkDocs Material documentation: </p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"Features/Text%20Formatting/","title":"Text Formatting","text":"<p>You can have lists like this</p> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <p>Or checklist lists to</p> <ul> <li> Get</li> <li> things</li> <li> done</li> </ul> <p>Also, get highlights and strikethroughs as above (similar to Obsidian).</p> <p>More formatting options for your webpage here. (but not compatible with Obsidian)</p>"},{"location":"RL%20study/2024-02-12/","title":"2024-02-12","text":""},{"location":"RL%20study/2024-02-12/#1-introduction","title":"1. Introduction","text":""},{"location":"RL%20study/2024-02-12/#11-sequential-decision-making","title":"1.1 Sequential decision making","text":""},{"location":"RL%20study/2024-02-12/#111-problem-definition","title":"1.1.1 Problem definition","text":"\\[ V_\\pi (s_0) = \\mathbb{E}_{p(a_0,s_1,a_1,\\cdots,a_T,s_T|s_0,\\pi)} \\bigg[ \\sum_{t=0}^T R(s_t,a_t)|s_0 \\bigg] \\] \\[\\begin{align*} p(a_0,s_1,a_1,\\cdots,a_T,s_T|s_0,\\pi) &amp;= \\pi(a_0|s_0)p_{\\text{env}}(o_1|a_0)\\delta(s_1=U(s_0,a_0,o_1)) \\\\ &amp;\\times \\pi(a_1|s_1) p_\\text{env}(o_2|a_1,o_1)\\delta(s_2=U(s_1,a_1,o_2)) \\\\ &amp; \\times \\pi(a_2|s_2)p_\\text{env}(o_3|a_{1:2},o_{1:2})\\delta(s_3=U(s_2,a_2,o_3))\\cdots \\end{align*}\\] <ul> <li>Remark</li> </ul> \\[ p_{S}(s_t|s_{t-1},a_{t-1})=\\sum_{\\lbrace o_t|s_t=U(s_{t-1},a_{t-1},o_t)\\rbrace} p_{\\text{env}}(o_t | a_{1:t-1}, o_{1:t-1}) \\] <ul> <li>Maximum expected utility (principle) and Optimal policy</li> </ul> \\[ \\pi^* = \\operatorname*{argmax}_\\pi \\mathbb{E}_{p_0(s_0)} [ V_\\pi(s_0)] \\]"},{"location":"RL%20study/2024-02-12/#112-universal-model","title":"1.1.2 Universal model","text":"<ul> <li> <p>hidden state \\(z_t\\) \ud639\uc740 world state</p> <ul> <li>update\ub294 non-determistic\ud558\uac8c \\(z_{t+1}=W(z_t,a_t,\\epsilon_t^z)\\) </li> </ul> </li> <li> <p>agent\ub294 potentially noisy \ud639\uc740 partial observation \\(o_{t+1}=O(z_{t+1},\\epsilon_{t+1}^o)\\) \ub97c \ubcf4\uac8c \ub428</p> <ul> <li>\\(o_t\\)\ub294 perceptual aliasing\uc774 \uc788\uc744 \uc218 \uc788\uc74c</li> </ul> </li> <li> <p>agent\uc758 . tate \\(s_t\\) \ub294 environment\uc758 \ubbff\uc74c(\ucd94\uce21)\uc744 \ub2f4\uace0 \uc788\uc74c. \ub530\ub77c\uc11c belief state \ub77c\uace0 \ubd80\ub974\uae30\ub3c4 \ud568.</p> <ul> <li>\ub0b4\ubd80\uc758 \ubbff\uc74c state\uac00 \uc788\uae30\uc5d0 perceptual aliasing\uc774 \uc788\ub294 \uc0c1\ud669\uc5d0\uc11c \uc678\ubd80\uc0c1\ud669\uc5d0 \ub300\ud55c \ucd94\uce21\uc73c\ub85c \uac19\uc740 observation \\(o\\)\uac00 \uad00\uce21\ub410\ub2e4 \ud558\ub354\ub77c\ub3c4 \ub0b4\ubd80 \ubbff\uc74c \uc0c1\ud669(agent\uc758 \ub0b4\ubd80 state \\(s_t\\))\uc5d0 \ub530\ub77c \ub2e4\ub978 \ud310\ub2e8(action)\uc744 \ub0b4\ub9b4 \uc218 \uc788\uac8c \ub428.</li> <li>\\(s_{t+1}=SU(s_t,a_t,o_{t+1})=U(s_{t+1|t},o_{t+1})=U(P(s_t,a_t),o_{t+1})\\) \uc640 \uac19\uc740 \uc2dd\uc73c\ub85c \ub0b4\ubd80 \ubbff\uc74c\uc774 update\ub428.<ul> <li>\\(SU\\): state update function, \\(P\\): prediction function, \\(U\\): update function</li> </ul> </li> </ul> </li> <li> <p>RL \ubc29\ubc95\ub860\uc5d0 \ub530\ub77c agent\uac00 \uc678\ubd80 observation\uc744 \uc608\uce21\ud558\ub824\ub294 \uc2dc\ub3c4\ub3c4 \ud568</p> <ul> <li>\\(\\hat{o}_{t+1}=D(s_{t+1|t})\\) \ub85c \ub2e4\uc74c observation\uc744 \ucd94\uce21\ud558\uace0 \uc774\ub97c decoder \\(D\\)\ub85c \ubaa8\ub378\ub9c1\ud558\uace0 \uc774\ub97c \ud6c8\ub828\uc5d0 \ud65c\uc6a9\ud558\uae30\ub3c4 \ud55c\ub2e4\uace0 \ud568(\uc544\ub9c8\ub3c4 Model-based RL\uc5d0\uc11c \ud558\ub294\ub4ef)</li> <li>\ucc38\uace0\ub85c model-based RL\uc5d0\uc11c \ubaa8\ub378\uc740 \uc678\ubd80\ud658\uacbd \ud639\uc740 World\uc5d0 \ub300\ud55c modeling\uc744 \ub9d0\ud558\ub294 \ub4ef<ul> <li>\uadf8\uc911 state transition or dynamics model \\(p_S(s'|s,a)\\) \ub97c \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\ub860\uc774 \uc788\ub294 \ub4ef</li> </ul> </li> </ul> </li> </ul> <p>Note</p> <p>On Representation Complexity of Model-based and Model-free Reinforcement Learning \uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c dynamics\ub97c \ud559\uc2b5\ud558\ub294 \ub4ef. \ub17c\ubb38\uc5d0 \ub530\ub974\uba74 <code>The sample complexity of learning the dynamics is less than the sample complexity of learning the policy.</code> \uc774\ub77c \ud55c\ub2e4.</p>"},{"location":"RL%20study/2024-02-12/#113-episodic-vs-continuing-task","title":"1.1.3 Episodic vs continuing task","text":"<code>Continuing task</code> \ub05d\uc774 agent\uc640 environment\uac00 \ub05d\uc5c6\uc774 \uc0c1\ud638\uc791\uc6a9 <code>Episodic task</code> \uc5b8\uc820\uac00\ub294 \ub05d\ub0a8. \ub05d\ub098\ub294 \uc2dc\uac04\uc774 \uc815\ud574\uc838 \uc788\uc73c\uba74 finite horizon problem. <ul> <li>reward-to-go \\(G_t\\) \ub294 random variable. </li> <li> <p>\uadf8 random variable\uc758 \\(\\pi\\)\uc5d0 \ub300\ud55c \uae30\ub313\uac12(\uc815\ud655\ud788\ub294 \\(s_t\\)\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \uae30\ub313\uac12)\uc73c\ub85c value function \\(V_\\pi (s_t)\\) \ub97c \uc815\uc758</p> </li> <li> <p>Discount factor \\(\\gamma \\in [0,1]\\)</p> <ul> <li>\\(\\gamma \\in [0,1)\\)\ub97c \uc4f0\ub294 \uc774\uc720\ub85c \uccab\ubc88\uc9f8\ub294 reward\uac00 bounded\uc77c \ub54c, value function\uc758 \uc218\ub834\uc131 \ubcf4\uc7a5\uc744 \uc704\ud574 \uc0ac\uc6a9, \ub450\ubc88\uc9f8\ub85c\ub294 short-term reward\uc5d0 \uac00\uc911\uce58\ub97c \uc8fc\ub824\uace0 \uc0ac\uc6a9, \uc989 \\(\\gamma =1\\)\uc778 \uacbd\uc6b0\ub294 \ud604\uc7ac\ub791 \uac00\uae4c\uc6b4 reward\ub098 \ubbf8\ub798\uc5d0 \ubc1b\ub294 reward\ub098 \uadf8 \uac12\uc774 \uac19\uc73c\uba74 \ub611\uac19\ub2e4\uace0 \uc0dd\uac01.</li> <li>\\(\\gamma\\)\uac00 \uc791\uc744 \uc218\ub85d \uadfc\uc2dc\uc548\uc801(myopic)\uc774 \ub428.  \ub530\ub77c\uc11c \\(1-\\gamma\\)\ub294 \ub2e4\uc74c step\uc5d0\uc11c \uc885\ub8cc\ub420 \uae30\ub300(\ud639\uc740 \ud655\ub960)\ub85c \ud574\uc11d\uac00\ub2a5</li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-12/#114-regret","title":"1.1.4  Regret","text":"<ul> <li>per-step regret at \\(t\\) : \\(l_t\\) <ul> <li>oracle policy\ub97c \uc54c\uc544\uc57c \uc368\uba39\uc744 \uc218 \uc788\uc744\ub4ef</li> </ul> </li> </ul> \\[ l_t := \\mathbb{E}_{s_{1:t}} \\big[R(s_t,\\pi_*(s_t))-\\mathbb{E}_{\\pi(a_t|s_t)}[R(s_t,a_t)]\\big] \\] <ul> <li> <p>simple regret at the last step, \\(l_T\\) </p> <ul> <li>simple regret\uc744 \ucd5c\uc801\ud654\ud558\ub294 \ubb38\uc81c\ub97c pure exploration\uc774\ub77c \ubd80\ub984</li> </ul> </li> <li> <p>cumulative regret (or total regret or just the regret) \\(L_T\\) </p> <ul> <li>\uc774\uac78\ub85c \ucd95\uc801\ud558\uba74\uc11c \ubaa8\ub378\uc774\ub791 policy\ub97c \ud559\uc2b5\ud558\uba74 <code>earning while learning</code> \uc774\ub77c \ud558\ub294\ub4ef</li> <li>exploration-exploitation tradeoff\ub97c \ud574\uacb0\ud558\uba74\uc11c \ubb38\uc81c\ub97c \ud480\uc5b4\uc57c \ud568</li> </ul> </li> </ul> \\[ L_T := \\mathbb{E}\\bigg[ \\sum_{t=1}^T l_t \\bigg] \\]"},{"location":"RL%20study/2024-02-12/#12-canonical-examples","title":"1.2 Canonical examples","text":""},{"location":"RL%20study/2024-02-12/#121-partially-observed-mdps-pomdp","title":"1.2.1 Partially observed MDPs (POMDP)","text":"<ul> <li>Figure 1.2\uc758 \uadf8\ub9bc\uc774 POMDP \ubaa8\ub378\uc774\ub77c \ud568</li> <li>environment dynamic model \uc744 \uc544\ub798\uc640 \uac19\uc774 stochastic transition function \\(W\\) \uc73c\ub85c </li> </ul> \\[ p(z_{t+1}|z_t, a_t) = \\mathbb{E}_{\\epsilon_t^z}[\\mathbb{I}(z_{t+1}=W(z_t,a_t,\\epsilon_t^z))] \\] <ul> <li>stochastic observation function \\(O\\) \uc740 </li> </ul> \\[ p(o_{t+1}|z_{t+1}) = \\mathbb{E}_{\\epsilon_{t+1}^o} [\\mathbb{I}(o_{t+1}=O(z_{t+1},\\epsilon_{t+1}^o))] \\] <p>\uc640 \uac19\uc774 \uc0ac\uc6a9\ud558\uc5ec <code>joint world model</code> \\(p_{\\text{WO}}(z_{t+1},o_{t+1}|z_t,a_t)\\) \uc744 \ub3c4\ucd9c</p> <p>\ub610\ub294 non-Markovian observation distribution \\(p_{\\text{env}} (o_{t+1}|o_{1:t}, a_{1:t})\\) \ub97c \uc544\ub798\uc640 \uac19\uc774 \uc815\uc758 </p> <ul> <li>\ub450 \uac1c\uc758 transition distribution \\(p(o|z), p(z'|z,a)\\) \uac00 world model\uc758 \uc815\ubcf4<ul> <li>\ub450 \ubd84\ud3ec\ub97c \uc548\ub2e4\uba74(world model \uc744 \uc548\ub2e4\uba74) \uc6d0\ub9ac\uc801\uc73c\ub85c\ub294 optimal policy\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4\uace0 \ud55c\ub2e4.</li> <li>agent\uc758 internal state \\(s_t\\) \ud639\uc740 belief state \\(b_t\\)\ub97c \\(s_t=b_t=p(z_t|h_t), \\ h_t=(o_{1:t}, a_{1:t-1})\\) \ub85c \ub450\uba74 \uad6c\ud560 \uc218 \uc788\ub2e4\uace0 \ud568</li> <li>belief state\ub294 \uc6d0\ub9ac\uc801\uc73c\ub85c(\ud1b5\uacc4\uc801\uc73c\ub85c) optimal policy\uc5d0 \ub300\ud574 \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc774\ub77c\uace0 \ud568.<ul> <li>\ub370\uc774\ud130\ub85c \ubd80\ud130 belief state\ub97c \uacc4\uc0b0\ud558\uba74 \uadf8 belief state\uac00 policy\uc758 parameter\uc640 \ub3c5\ub9bd\uc801\uc774\ub77c\ub294 \ub73b</li> <li>\uadf8\ub7ec\ub098 computing belief state is wildly intractible \uc774\ub77c \ud568.</li> </ul> </li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-12/#122-markov-decision-process-mdps","title":"1.2.2 Markov decision process (MDPs)","text":"<ul> <li> <p>MDP\ub294 special case of POMDP</p> <ul> <li>\\(z_t=o_t=s_t\\) \uc778 \uc0c1\ud669\uc744 \uac00\uc815</li> <li>\ub530\ub77c\uc11c <code>state transition matrix induce by the world model</code> : \\(p_S(s_{t+1}|s_t,a_t):=\\mathbb{E}_{\\epsilon_t^s}[\\mathbb{I} (s_{t+1}=W(s_t,a_t,\\epsilon_t^s))]\\) \ub85c \uc815\uc758\ud558\uc5ec MDP\ub97c \uae30\uc220</li> </ul> </li> <li> <p>POMDP \uc5d0\uc11c \uc0ac\uc6a9\ub418\ub358 observation \uad00\uc810\uc5d0\uc11c MDP\uc5d0\uc11c\ub294 observation\uc73c\ub85c reward\ub97c environment\uc5d0\uac8c\uc11c \ubc1b\ub294\ub2e4\uace0 \uc0dd\uac01\ud55c\ub2e4.</p> <ul> <li>expected reward(\uc0ac\uc2e4 \uc870\uac74\ubd80 \uae30\ub313\uac12)\ub294 \ub450\uac00\uc9c0 \ud615\ud0dc\uac00 \uc788\uace0 \ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758  </li> </ul> </li> <li>MDP\uc5d0\uc11c state, action \uc9d1\ud569\uc774 finite\ud558\uba74 tabular representation \uc73c\ub85c MDP\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc73c\uba70 \uc774\ub807\uac8c \ud45c\ud604\ub418\ub294 MDP\ub97c finite state machine \uc774\ub77c \ubd80\ub978\ub2e4.<ul> <li>\uc774 \uacbd\uc6b0\uc5d0 \\(p_S, p_R\\) \uc744 \uc54c \uace0 \uc788\uc73c\uba74 dynamic programming \ud14c\ud06c\ub2c9\uc73c\ub85c optimal policy \ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. (Section 2.2 \uc5d0\uc11c \ub2e4\ub8ec\ub2e4.)</li> <li>\uadf8\ub7f0\ub370 \ub300\ubd80\ubd84 \uc774 \ub450 distribution (world model) \uc744 \ubaa8\ub974\uae30 \ub54c\ubb38\uc5d0 DP \ubc29\ubc95\uc73c\ub85c \ucd5c\uc801 \uc815\ucc45\uc774 \ud480\ub9ac\ub294 \uacbd\uc6b0\ub294 \uc798 \uc5c6\uace0 RL \ud14c\ud06c\ub2c9\uc744 \uc0ac\uc6a9\ud558\uc5ec good policy \ub97c \uad6c\ud558\uae30 \uc704\ud574 \ub178\ub825\ud55c\ub2e4.</li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-12/#123-contextual-mdps","title":"1.2.3 Contextual MDPs","text":"<ul> <li>Contextual MDP\ub294 environment\uc758 dynamics\uc640 reward\uac00 hidden static parameter(context\ub77c \ubd80\ub984)\uc5d0 \uc758\uc874\ub418\ub294 MDP \ubaa8\ub378\uc744 \ub73b\ud55c\ub2e4.</li> </ul>"},{"location":"RL%20study/2024-02-12/#124-contextual-bandits","title":"1.2.4 Contextual bandits","text":""},{"location":"RL%20study/2024-02-12/#125-belief-state-mdps","title":"1.2.5 Belief state MDPs","text":"<ul> <li>state\uac00 \ud655\ub960\ubd84\ud3ec (belief state or information state)</li> <li> <p>belief state MDP (w/ deterministic dynamics) </p> </li> <li> <p>reward function </p> </li> <li> <p>\uc774 (PO)MDP\ub97c \ud47c\ub2e4\uba74, \uc6b0\ub9ac\ub294 exploration-exploitation problem\uc758 optimal solution\uc744 \uc5bb\uac8c \ub41c\ub2e4.</p> </li> </ul>"},{"location":"RL%20study/2024-02-12/#126-optimization-problems","title":"1.2.6 Optimization problems","text":"<p>bandit \ubb38\uc81c\ub294 agent\uac00 \uc5d0\uc774\uc804\ud2b8\uac00 \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\uae30 \uc704\ud574 \uc138\uacc4\uc640 \uc0c1\ud638\uc791\uc6a9\ud574\uc57c \ud558\uc9c0\ub9cc \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \ud658\uacbd\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294 \ubb38\uc81c\uc758 \uc608\uc785\ub2c8\ub2e4. (??? \uc0c1\ud638\uc791\uc6a9\ud558\uc9c0 \uc54a\uc73c\uba74 \ud658\uacbd\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294\ub2e4?)</p> <ul> <li>\uc989, agent \uc758 internal belief state \ub294 \uc2dc\uac04\uc5d0 \ub530\ub77c \ubcc0\ud654\ud558\uc9c0\ub9cc environment state\ub294 \ubcc0\ud558\uc9c0 \uc54a\ub294\ub2e4!</li> <li>\uc774\ub7ec\ud55c \uc0c1\ud669\uc740 \\(R\\) \uc774 \uace0\uc815\ub41c \ubaa8\ub974\ub294 \ud568\uc218\uc77c \ub54c, \uc774 \ud568\uc218\ub97c optimize\ud560 \ub54c \ubc1c\uc0dd\ud55c\ub2e4.</li> <li>\uc6b0\ub9ac\ub294 \\(R\\)\uc758 evaluation\uc744 <code>query the function</code> \uc774\ub77c\ub294 \uc6a9\uc5b4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\uc2dc\ud55c\ub2e4.</li> <li>agent\uc758 \ubaa9\ud45c\ub294 \ucd5c\ub300\ud55c \uc801\uac8c querying\ud558\uba74\uc11c \\(R\\)\uc758 optimum\uc744 \ucc3e\ub294 \uac83\uc774\ub2e4.</li> </ul>"},{"location":"RL%20study/2024-02-12/#1261-best-arm-identification","title":"1.2.6.1 Best-arm identification","text":"<ul> <li>\ud45c\uc900\uc801\uc778 multi-armed bandit \ubb38\uc81c\uc5d0\uc11c\ub294 sum of expected rewards\ub97c \ucd5c\ub300\ud654\ud558\ub294 \uac8c \ubaa9\ud45c\uc774\ub2e4.</li> <li>\uc5b4\ub5a4 \ubb38\uc81c \uac19\uc740 \uacbd\uc6b0\ub294 \\(T\\) trials\uac00 \uace0\uc815\ub418\uc5b4 \uc788\uc744 \ub54c best arm\uc744 \uc120\ud0dd\ud558\ub294 \ubb38\uc81c\uac00 \ubaa9\ud45c\ub85c \ub420 \uc218 \uc788\ub2e4.<ul> <li>\uc774\ub97c best-arm identification \uc774\ub77c \ud55c\ub2e4. Formally, optimizing the final reward criterion:</li> </ul> </li> </ul> \\[ V_{\\pi,\\pi_T} = \\mathbb{E}_{p(a_{1:T},r_{1:T}|s_0,\\pi)}[R(\\hat{a})] \\] <p>\uc5ec\uae30\uc11c \\(\\hat{a}=\\pi_T (a_{1:T}, r_{1:T})\\)\ub294 estimated optimal arm computed by the terminal policy \\(\\pi_T\\) \uc774\ub2e4.  - \uc774\ub7ec\ud55c \ubb38\uc81c\ub294 standard bandits\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ubc29\ubc95\ub860\uc758 \uac04\ub2e8\ud55c adapation\uc73c\ub85c \ud480\ub9b0\ub2e4\uace0 \ud55c\ub2e4.</p>"},{"location":"RL%20study/2024-02-12/#1262-bayesian-optimization","title":"1.2.6.2 Bayesian optimization","text":"<ul> <li>Bayesian optimization \uc740 gradient-free approach to optimizing expensive blackbox function</li> </ul> \\[ w^* =\\operatorname*{argmax}_w R(w) \\] <ul> <li>\\(R\\) \uc740 unknown function\uc774\uace0 \ucd5c\uc18c\ud55c\uc758 action \ub9cc\uc744 \uc774\uc6a9\ud558\uae30\ub97c \ubc14\ub78c(action \uc740 \\(R\\)\uc758 evaluation\uc5d0 \uc0ac\uc6a9)</li> <li>belief state\uac00 \ud568\uc218 \\(R\\)\uc758 \ubd84\ud3ec\ub97c \uac00\uc9c4\ub2e4\uace0 \uac00\uc815(\\(s_t\\sim p(R|h_t)\\))</li> <li>\uadf8\ub9ac\uace0 \uadf8 \ubd84\ud3ec \\(s_t\\)\uc77c \ub54c \uc815\ud574\uc9c0\ub294 \\(R\\)\uc5d0 \ub300\ud574 \\(w_t=\\pi (s_t)\\)\ub77c\ub294 policy\ub97c \uc815\ud558\ub294\ub4ef</li> <li>Bayesian optimization\uc740 <code>infinite arm</code> version of the best-arm identification \ubb38\uc81c\ub77c\uace0 \ubcfc \uc218 \uc788\uc74c(discrete choice of arm \\(a \\in \\{ 1,\\cdots, K\\}\\))</li> </ul>"},{"location":"RL%20study/2024-02-12/#1263-active-learning","title":"1.2.6.3 Active learning","text":"<p>\uc704\uc758 \\(R\\)\uc744 \ucd5c\ub300\ud654\ud558\ub294 \\(w^*\\) \ub97c \ucc3e\ub294 \uac83\uc774 BayesOpt \ubc29\ubc95\uc774\uc5c8\ub2e4\uba74 active learning\uc5d0\uc11c\ub294 \uc11c\ub85c \ub2e4\ub978 \\(w_t\\) \ub97c \uac00\uc9c0\uace0 evaluating\ud558\uba74\uc11c \uc54c\ub824\uc9c0\uc9c0 \uc54a\uc740 entire function \\(R\\) \uc744 \ud559\uc2b5\ud558\uace0\uc790 \ud55c\ub2e4.</p> <ul> <li>\ucd5c\uc801\uc758 \uc804\ub7b5(optimal strategy)\uc740 \ubbf8\uc9c0 \ud568\uc218 \\(R\\) \uc5d0 \ub300\ud55c belief state\ub97c \ucde8\ud558\uc9c0\ub9cc(\ud655\ub960\ubd84\ud3ec\ub85c \ubcf4\uc9c0\ub9cc), \uc774\uc81c \ucd5c\uc801\uc758 \uc815\ucc45\uc740 belief state\uc758 entropy\ub97c \uc904\uc774\uae30 \uc704\ud574 query point \\(w_t\\)\ub97c \uc120\ud0dd\ud558\ub294 \ub4f1 \ub2e4\ub978 \ubc29\ubc95\ub860\uc73c\ub85c \\(R\\)\uc744 \ucd5c\ub300\ud654\ud55c\ub2e4.<ul> <li>R\uc758 \ud655\uc2e4\uc131?\uc744 \ub192\uc774\ub294 \uc2dd\uc73c\ub85c R\uc744 \uc54c\ub824\uace0 \ud558\ub294\ub4ef</li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-12/#1264-stochastic-gradient-descent-sgd","title":"1.2.6.4 Stochastic Gradient Descent (SGD)","text":"<p>SGD\ub97c \uc0ac\uc6a9\ud558\uc5ec sequential decision making process \ub97c \uad6c\uc131\ud560 \uc218 \uc788\ub2e4.</p> <ul> <li>action space\ub294 querying the unknown function \\(R\\) at locations \\(a_t=w_t\\) \ub85c \uad6c\uc131</li> <li>reward \\(r_t = R(w_t)\\) </li> <li>BayesOpt\uc640 \ub2e4\ub974\uac8c gradient \\(g_t = \\nabla_w R(w)|_{w_t}\\) \ub97c \uad00\ucc30\ud55c\ub2e4.</li> <li>Environment state \ub294 true function \\(R\\) \uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uac00\uc9c0\uace0 \uc788\ub294\ub370 \uc774 \\(R\\)\uc740 agent\uc758 action\uc5d0 \ub530\ub978 observation(gradient\uc640 reward)\uc744 \uc0dd\uc131\ud558\ub294\ub370 \uc0ac\uc6a9\ub41c\ub2e4.</li> <li>Agent state\ub294 parameter \\(w_t\\) \uac00 \uc0ac\uc6a9\ub418\uace0, \ucd94\uac00\uc801\uc73c\ub85c Adam \uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 first and second moment \\(m_t, v_t\\)\uac00 \ud568\uaed8 \uc0ac\uc6a9\ub418\uae30\ub3c4 \ud55c\ub2e4. (\uc989, \\(s_t = (w_t, m_t, v_t)\\))</li> <li>optimizer rule(Adam or SGD or etc.)\uc744 \ub530\ub77c \\(w_t\\)\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\uac8c \ub418\uace0 \uadf8 \uc5c5\ub370\uc774\ud2b8 \ub420\ub54c\uc758 learning rate \\(\\alpha_t\\)\uac00 policy\uc5d0 \uc758\ud574 \uacb0\uc815\ub41c\ub2e4. (\uc989, \\(\\alpha = \\pi(s_t)\\))</li> </ul>"},{"location":"RL%20study/2024-02-12/#134-dealing-with-partial-observability","title":"1.3.4 Dealing with partial observability","text":""},{"location":"RL%20study/2024-02-12/#1341-optimal-solution","title":"1.3.4.1 Optimal solution","text":"<ul> <li>world\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uac83(\\(p(o|z), p(z'|z,a)\\))\uc744 \uc54c \ub54c Bayesian inference\ub97c \ud1b5\ud574 \ub2f5\uc744 \uc6d0\ub9ac\uc801\uc73c\ub85c\ub294 \uad6c\ud560 \uc218 \uc788\ub2e4.(\uc774 \uacbd\uc6b0\ub294 \ubaa8\ub4e0 history\ub97c \uc54c\uace0 \uac00\ub294 case\uc778 \uac83 \uac19\ub2e4.)</li> <li>\uadf8\ub7ec\ub098 \ubcf4\ud1b5 intractible\ud55c \uacbd\uc6b0\uac00 \ub9ce\uc544 simpler approximation\uc774 \uc0ac\uc2e4\uc0c1 \uc0ac\uc6a9\ub41c\ub2e4.</li> <li>\\(p(o_{t+1}|h_t,a_t)\\) \uc5d0 \ub300\ud574 o\ub97c \ud0c0\uac9f\ud558\uc5ec world latent state\uc5c6\uc774 \ud559\uc2b5\ud558\ub824\ud558\ub294 \uac78 <code>predictive state representation (PSR)</code> \ub77c\uace0 \ubd80\ub984</li> <li><code>observable operator models</code>\uc758 \uc544\uc774\ub514\uc5b4 \uadf8\ub9ac\uace0 <code>successor representations</code>\uc640 \uad00\ub828\uc774 \uc788\uc74c</li> </ul>"},{"location":"RL%20study/2024-02-12/#1342-finite-observation-history","title":"1.3.4.2 Finite observation history","text":"<ul> <li>last \\(k\\) observation\ub9cc \uac00\uc9c0\uace0 \ud558\ub294 \ubaa8\ub378</li> <li>\\(o\\)\uac00 image\uc778 \uacbd\uc6b0\uc5d0 frame stacking\uc774\ub77c \ud55c\ub2e4.</li> <li>\uc77c\ubc18\uc801\uc778 MDP \ubc29\ubc95\uc73c\ub85c \ub2f5\uc744 \ucc3e\uc544 \ub098\uac04\ub2e4.</li> </ul>"},{"location":"RL%20study/2024-02-12/#1343-stateful-recurrent-policies","title":"1.3.4.3 Stateful (recurrent) policies","text":"<ul> <li>\uac00\uc7a5 \uac15\ub825\ud55c \uc811\uadfc\ubc95\uc73c\ub85c entire past\uc5d0 \ub300\ud55c \uc815\ubcf4 \ubaa8\ub450\ub97c \uc774\uc6a9\ud55c\ub2e4.</li> <li>policy\ub97c RNN\uc73c\ub85c \ubaa8\ub378\ub9c1\ud55c\ub2e4. (\uc774\ub294 R2D2 paper\uc5d0 \uc0ac\uc6a9\ub418\uc5c8\ub2e4.)</li> <li>RNN\uc758 hidden state \\(z_t\\)\uac00 past observation\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c implicitly summarize\ud55c\ub2e4\uace0 \uc0dd\uac01\ud55c\ub2e4.</li> <li>explicit notion of belief state or uncertainty\uc5d0 \ub300\ud55c \uac83\uc774 \uc5c6\uae30 \ub54c\ubb38\uc5d0 <code>information-gathering actions</code>\ub97c \uc218\ud589\ud558\ub824\ub294 \uacc4\ud68d\uc774 \uc77c\uc5b4\ub098\uc9c0 \uc54a\ub294\ub2e4.<ul> <li>\uadf8\ub7ec\ub098 meta-learning\uc758 \ubc29\ubc95\ub860\uc73c\ub85c \uc774\ub7ec\ud55c \uac83\uc744 \ud574\uacb0\ud558\ub824 \ud55c\ub2e4.</li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-19/","title":"2024-02-19","text":""},{"location":"RL%20study/2024-02-19/#14-exploration-exploitation-tradeoff","title":"1.4 Exploration-exploitation tradeoff","text":"<p>\ud604\uc7ac \ub2f9\uc7a5 \ud070 reward\ub97c \uc704\ud574 \uc120\ud0dd\ud560 \uac83\uc778\uc9c0 vs \ub354 \ub113\uc740 state-action space\ub97c \ud65c\ubcf4\ud568\uc5d0 \ub530\ub77c \uad6c\ud574\uc9c8 \uc218\ub3c4 \uc788\ub294 \ub354 \ud070 reward\ub97c \ubc14\ub784 \uac83\uc778\uc9c0</p>"},{"location":"RL%20study/2024-02-19/#141-simple-heuristics","title":"1.4.1 Simple heuristics","text":"<ul> <li>Pure exploitation (greedy policy) \\(a_t = \\operatorname*{argmax}_a Q(s,a)\\) </li> <li>\\(\\epsilon\\)-greedy policy \\(\\pi_\\epsilon\\) parametrized by \\(\\epsilon \\in [0,1]\\) <ul> <li>\\(1-\\epsilon\\)\uc758 \ud655\ub960\ub85c \\(a_t=\\operatorname*{argmax}_a \\hat{R}_t(s_t,a)\\)</li> <li>\\(\\epsilon\\)\uc758 \ud655\ub960\ub85c random action</li> <li>\uc774 \ubc29\ubc95\uc740 agent\uac00 all state-action \uc870\ud569\uc5d0 \ub300\ud574 \uacc4\uc18d exploration\uc744 \ud558\uac8c \ud574 \uc90c.</li> <li>\uadf8\ub7ec\ub098 \uc774 \ubc29\ubc95\ub860\uc740 <code>suboptimal</code>\uc774\ub2e4. \uc65c\ub0d0\ud558\uba74 \\(\\epsilon/ |\\mathcal{A}|\\)\uc758 \ud655\ub960\ub85c random action\uc744 \uace0\ub974\uae30 \ub54c\ubb38\uc774\ub2e4.<ul> <li>\ub530\ub77c\uc11c \\(\\epsilon\\)\uc744 \uc2dc\uac04\uc5d0 \ub530\ub77c 0\uc73c\ub85c annealing\ud558\ub294 \ubc29\ubc95\uc744 \ud1b5\ud574 \uc774\ub7ec\ud55c suboptimal \ud604\uc0c1\uc744 \uc644\ud654\ud560 \uc218 \uc788\ub2e4.</li> </ul> </li> <li>\uc774 \ubc29\ubc95\uc758 \ub610 \ub2e4\ub978 \ubb38\uc81c\uc810\uc740 <code>dithering</code>\uc774\ub77c\ub294 \uac83\uc778\ub370, \uc774\ub294 agent\uac00 \uacc4\uc18d \ud574\uc11c \ubb34\uc5c7\uc744 \ud560\uc9c0 \ub9c8\uc74c\uc744 \ubc14\uafbc\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4.<ul> <li>\uc774\uac78 \ud574\uacb0\ud558\uae30 \uc704\ud574\uc11c \\(\\epsilon z\\)-greedy \ubc29\ubc95\ub860(Temporally-Extended \u03b5-Greedy Exploration)\uc774 \uc81c\uc2dc\ub418\uc5c8\ub294\ub370 \ubc29\ubc95\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. (FYI: \ubc1c\uc74c\uc740 easy-greedy)<ul> <li>\\(\\epsilon\\)-greedy\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \\(1-\\epsilon\\)\uc758 \ud655\ub960\ub85c exploitation\uc744 \ud55c\ub2e4.</li> <li>\\(\\epsilon\\)\uc758 \ud655\ub960\ub85c\ub294 \\(n\\)\ubc88 \uac19\uc740 action\uc744 \ubc18\ubcf5\ud55c\ub2e4.</li> <li>\\(n\\)\uc740 exponential, uniform, zeta distribution \uc911\uc5d0\uc11c sampling\ub41c\ub2e4. (\ub17c\ubb38\uc5d0\uc11c\ub294 zeta distribution with \\(\\mu=2\\)\uac00 \uc77c\uad00\uc131\uc788\uac8c \uc88b\uc740 \uacb0\uacfc\uac00 \ub098\uc628\ub2e4\uace0 \uc2e4\ud5d8\uc801\uc73c\ub85c \ubcf4\uc600\ub2e4.)</li> <li>\uc774 \ubc29\ubc95\ub860\uc758 \uba54\uc778 \uac00\uc124 \ud639\uc740 \uc8fc\uc7a5\uc740 \\(\\epsilon\\)-greedy\uc758 \uc8fc\ub41c \ub2e8\uc810\uc774 temporal persistence\uac00 \ubd80\uc871\ud558\ub2e4\ub294 \uac83\uc774\ub2e4. \uc774\ub807\uac8c \uac19\uc740 action\uc73c\ub85c \ucb49 \uac19\uc740 \ud589\ub3d9\uc744 \uc5b4\ub290\uc815\ub3c4 \ud574\uc918\uc57c local optima\ub97c \ud0c8\ucd9c\ud560 \uc218 \uc788\ub2e4\uace0 \uc8fc\uc7a5\ud55c\ub2e4. </li> <li>\ub610\ud55c \uc774 \ubc29\ubc95\ub4e4\uc740 \uc57c\uc0dd\uc758 \ub3d9\ubb3c \uc138\uacc4\uc5d0\uc11c \ub3d9\ubb3c\ub4e4\uc774 \uba39\uc774 \ucc44\uc9d1\uc744 \ud558\ub7ec \ubc29\ud5a5\uc744 \uc815\ud560 \ub54c \ub79c\ub364\ud558\uac8c \ud558\ub098\uc758 \ubc29\ud5a5\uc744 \uc815\ud574\uc11c \uc5b4\ub290\uc815\ub3c4 heavy-tailed distribution\uc744 \ub530\ub974\ub294 duration\ub9cc\ud07c \uacc4\uc18d \uc804\uc9c4\ud558\ub294 ecological \ubaa8\ub378\uc774 optimal \ub3d9\ubb3c \ucc44\uc9d1 \uc804\ub7b5\uc774\ub77c\ub294 \uc0ac\uc2e4\uc774\ub77c\ub294 \uac83\uc774 \uc54c\ub824\uc84c\uae30\uc5d0 \uc774\uc5d0 motivate\ub41c \ubc29\ubc95\ub860\uc774\ub77c \ud560 \uc218 \uc788\ub2e4.</li> </ul> </li> </ul> </li> <li>\\(\\epsilon\\)-greedy\uac00 \uc544\ub2cc \ub610 \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c <code>Boltzmann exploration</code>\uc774\ub77c\ub294 \uac83\uc774 \uc788\ub2e4. \uba54\uc778 \uc544\uc774\ub514\uc5b4\ub294 \ub2f9\uc7a5\uc758 reward\uac00 \ub192\uc740 action\uc5d0 \ub192\uc740 \ud655\ub960\uc744 \ubd80\uc5ec\ud558\ub294 \uac83\uc774\ub2e4.<ul> <li>\uc774 \ubc29\ubc95\ub860\uc740 reward estimate\uc758 \uacb0\uacfc\uc5d0 \ub530\ub77c policy\uc758 \ubcc0\ud654\uac00 \uc880\ub354 smoother\ud574\uc9c0\ub294 \uac83\uc774\ub2e4.</li> </ul> </li> </ul> </li> </ul> \\[ \\pi_\\tau(a|s) = \\dfrac{\\exp (\\hat{R}_t(s_t,a)/\\tau)}{\\sum_{a'}\\exp (\\hat{R}_t (s_t,a')/\\tau)} \\] <p>\\(\\tau\\)\uac00 0\uc73c\ub85c \uac00\uba74 greedy policy\uac00 \uac00\uae4c\uc6cc\uc9c0\uace0, \\(\\tau\\)\uac00 \ubb34\ud55c\ub300\ub85c \uac00\uba74 uniform\uc5d0 \uac00\uae4c\uc6cc\uc9c4\ub2e4. (\uc544\ub798 \uadf8\ub9bc\uc740 \uc624\ud0c0\uac00 \uc788\ub294 \ub4ef \ud568)</p> <p></p> <ul> <li>\ub9c8\uc9c0\ub9c9\uc73c\ub85c \ub610\ub2e4\ub978 \uc811\uadfc \ubc95\uc73c\ub85c\ub294 outcome\uc758 \uacb0\uacfc(consequences of the outcome)\uac00 uncertain\ud55c (state, action)\uc744 \ub354 explore\ud558\ub3c4\ub85d \ud558\ub294 \ubc29\ubc95\ub860\uc774 \uc788\ub2e4.<ul> <li>\uc774 \ubc29\ubc95\uc740 <code>exploration bonus</code> \\(R_t^b(s,a)\\)\ub97c \ud1b5\ud574 \uc774\ub8e8\uc5b4\uc9c0\ub294\ub370, \uc774\ub294 state s\uc77c \ub54c action a\ub97c \uc2e4\uc2dc\ud55c \ud69f\uc218\uac00 \uc801\uc744 \ub54c \ud070 \uac12\uc744 \ubd80\uc5ec\ud558\ub3c4\ub85d bonus\uac00 \uc124\uacc4\ub41c\ub2e4.</li> <li>\uc774 bonus\ub97c \uc6d0\ub798\uc758 reward\uc5d0 \ub354 \ud558\uc5ec \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub807\uac8c \ud558\uc5ec agent\uac00 world\uc5d0 \ub300\ud574 \ub354 \ud0d0\uc0c9\ud558\uc5ec useful information\uc744 \uc5bb\ub3c4\ub85d \ud558\ub294 \ud6a8\uacfc\uac00 \uc788\ub2e4. \uc774\ub7ec\ud55c reward\ub97c <code>intrinsic reward</code> \ud568\uc218\ub77c\uace0 \ubd80\ub978\ub2e4. (Section 5.2.4\uc5d0\uc11c \ub354 \uc790\uc138\ud788 \ub2e4\ub8e8\ub294\ub4ef)</li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-19/#142-methods-based-on-the-belief-state-mdp","title":"1.4.2 Methods based on the belief state MDP","text":"<p>Bayesian approach\ub97c \uc801\uc6a9\ud558\uc5ec exploration-exploitation tradeoff\uc5d0 \ub300\ud55c optimal solution\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\ub294\ub370 \uadf8\uc5d0 \ub300\ud574 \ub2e4\ub8e8\ub294 section\uc774\ub2e4.</p>"},{"location":"RL%20study/2024-02-19/#1421-bandit-case-gittins-indices","title":"1.4.2.1 Bandit case (Gittins indices)","text":"<p>\ubaa8\ub378 \ud30c\ub77c\uba54\ud130\uc5d0 \ub300\ud55c belief state, \\(p(\\theta_t|\\mathcal{D}_{1:t})\\)\ub97c recursive\ud558\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uc790. \uc774 \uc0c1\ud669\uc758 belief state MDP\uc5d0\uc11c \uc6b0\ub9ac\ub294 policy\ub97c \uc5b4\ub5bb\uac8c \uacc4\uc0b0\ud558\ub294\uac00(\uad6c\ud558\ub294\uac00)?</p> <p>context-free bandits with a finite number of arms\uc778 \uacbd\uc6b0\uc5d0\ub294 optimal policy\ub97c dynamic programming\uc744 \ud1b5\ud574 \uad6c\ud560 \uc218 \uc788\ub2e4.</p> <ul> <li>\uadf8 \uacb0\uacfc\ub294 \ub9e4 \uc2a4\ud15d\ubcc4 table of action probabilities \\(\\pi_t(a_1,\\cdots,a_K)\\) \uc73c\ub85c \ud45c\ud604\ub420 \uc218 \uc788\ub294\ub370, \uc774\ub97c <code>Gittins indices</code>\ub77c \ubd80\ub978\ub2e4.</li> <li>\ubb3c\ub860 general contextual bandits\uc5d0 \ub300\ud55c optimal policy\ub294 intractable\ud558\ub2e4.</li> </ul> <p></p>"},{"location":"RL%20study/2024-02-19/#1422-mdp-case-bayes-adaptive-mdps","title":"1.4.2.2 MDP case (Bayes Adaptive MDPs)","text":"<p>\uc704 \ubc29\ubc95\ub860\uc744 MDP case\uc5d0 \uc801\uc6a9\ud558\uc5ec BAMDP(Bayes Adaptive MDP)\ub85c \ubc14\uafd4\uc11c \uc801\uc6a9\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub7ec\ud55c \ud655\uc7a5\uc801\uc6a9 \ubc29\ubc95\ub860\uc774 computationally intractable to solve\ud55c \uac83\uc774 \uc54c\ub824\uc838 \uc788\uc5b4\uc11c \ub2e4\uc591\ud55c \uadfc\uc0ac \ubc29\ubc95\uc73c\ub85c BADMP\ub97c \ud47c\ub2e4\uace0 \ud55c\ub2e4.</p>"},{"location":"RL%20study/2024-02-19/#143-upper-confidence-bounds-ucbs","title":"1.4.3 Upper confidence bounds (UCBs)","text":"<p>explore-exploit\uc5d0 \ub300\ud55c optimal solution\uc740 intractable\ud558\ub2e4. \ud558\uc9c0\ub9cc \uc9c1\uad00\uc801\uc778 \ubc29\ubc95(\uc6d0\ub9ac)\uc5d0 \uae30\ucd08\ud55c \ubc29\ubc95\ub860\uc774 \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294\ub370, \uadf8 \uc6d0\ub9ac\ub294 <code>optimism in the face of uncertainty (OFU)</code> \ub77c\ub294 \uac83\uc774\ub2e4. \uc774 \uc6d0\ub9ac\ub294 action\uc744 greedy\ud558\uac8c \uc120\ud0dd\ud558\ub294\ub370, \uadf8 \uc120\ud0dd\uc758 \uadfc\uac70\uac00 optimistic estimate of reward\ub85c \uc778\ud55c \uc120\ud0dd\uc774 \ub418\ub3c4\ub85d \ud558\ub294 \uac83\uc774\ub2e4. \uc774 \ubc29\ubc95\ub860\uc774 optimal\ud558\ub2e4\ub294 \uac83\uc740 <code>R-max</code> paper\uc5d0 \uc99d\uba85\ub418\uc5b4 \uc788\ub294\ub370, \uc774 \uc99d\uba85\uc740 \uc120\ud589 \uc5f0\uad6c\uc600\ub358 <code>E3</code> paper\uc758 \uacb0\uacfc\uc5d0 \uae30\ucd08\ud558\uc5ec \uc99d\uba85\ub418\uc5c8\ub2e4.</p> <p>\uc774 \uc6d0\ub9ac\uc758 \uac00\uc7a5 \uc77c\ubc18\uc801\uc778 implementation\uc740 <code>upper confidence bound (UCB)</code>\ub77c\ub294 \uac1c\ub150\uc5d0 \uadfc\uac70\ud558\uc5ec \uc774\ub8e8\uc5b4\uc9c0\ub294\ub370, \uc6b0\ub9ac\ub294 bandit case\uc5d0\uc11c \uc774\ub97c \uc124\uba85\ud558\uace0, \uadf8 \ub2e4\uc74c\uc5d0 MDP case\uc5d0 \ub300\ud574 \ub2e4\ub8e8\uaca0\ub2e4.</p>"},{"location":"RL%20study/2024-02-19/#1431-basic-idea","title":"1.4.3.1 Basic idea","text":"<ul> <li>UCB \ubc29\ubc95\uc744 \uc4f0\uae30 \uc704\ud574\uc11c\ub294 agent\uac00 optimistic reward function\uc744 \uc0ac\uc6a9\ud55c\ub2e4\uace0 \uac00\uc815\ud55c\ub2e4. \uc989, \ub192\uc740 \ud655\ub960\ub85c \\(\\tilde{R}_t(s_t,a) \\geq R(s_t,a) \\ (\\forall a\\in \\mathcal{A})\\) \uc778 \uc0c1\ud669\uc5d0\uc11c \ub2e4\uc74c\uacfc \uac19\uc740 greedy action\uc744 \ucde8\ud55c\ub2e4.</li> </ul> \\[ a_t = \\operatorname*{argmax}_a \\tilde{R}_t (s_t,a)  \\] <ul> <li>UCB\ub294 optimistic estimate\uac00 exploration\uc744 \uc7a5\ub824\ud558\ub294 <code>exploration bonus</code>\uc758 \ud615\ud0dc\ub85c \ubcfc \uc218 \uc788\ub2e4.</li> <li>\uc77c\ubc18\uc801\uc73c\ub85c the amount of optimism, \\(\\tilde{R}_t-R\\)\uc774 \uc2dc\uac04\uc5d0 \ub530\ub77c \uac10\uc18c\ud558\uba70, \ub530\ub77c\uc11c agent\ub294 exploration\uc744 \uc904\uc774\uac8c \ub41c\ub2e4.</li> <li>\uc801\uc808\ud558\uac8c \uc124\uacc4\ub41c optimistic reward estimate \\(\\tilde{R}\\)\uc744 \ud1b5\ud574 UCB \ubc29\ubc95\ub860\uc774 \ub9ce\uc740 variants of bandits\uc5d0\uc11c near-optimal regret\uc744 \uac00\uc9c4\ub2e4\ub294 \uac83\uc774 \uc54c\ub824\uc838 \uc788\ub2e4.<ul> <li>optimistic function \\(\\tilde{R}\\)\uc740 \ub2e4\uc591\ud55c \ubc29\ubc95\uc73c\ub85c \uc124\uacc4\ub420 \uc218 \uc788\ub294\ub370 \ub2e4\uc74c section\ub4e4\uc5d0\uc11c \ub2e4\ub8ec\ub2e4.</li> </ul> </li> </ul>"},{"location":"RL%20study/2024-02-19/#1432-bandit-case-frequentist-approach","title":"1.4.3.2 Bandit case: Frequentist approach","text":"<ul> <li>Frequentist approach\uc5d0\uc11c\ub294 confidence bound\ub97c \uacc4\uc0b0\ud560 \ub54c estimation error\uc758 \ub192\uc740 \ud655\ub960\uc758 \uc0c1\ud55c(upper bound)\uc744 \uc720\ub3c4\ud558\uae30 \uc704\ud574 <code>concentratioin inequality</code> \ub97c \uc0ac\uc6a9\ud55c\ub2e4.</li> </ul> \\[ | \\hat{R}_t (s,a)-R_t(s,a)| \\leq \\delta_t (s,a) \\] <ul> <li>\\(\\hat{R}_t\\) : usua estimate of \\(R\\) (often the MLE), \\(\\delta_t\\) : properly selected function</li> <li>Optimistic reward \\(\\tilde{R}_t (s,a)= \\hat{R}_t(s,a) +\\delta_t(s,a)\\) \ub85c \uc124\uc815\ud55c\ub2e4.</li> </ul> <p>Examples</p> <p>context-free Bernoulli bandit, \\(R(a)\\sim \\text{Ber}(\\mu(a))\\)  MLE \\(\\hat{R}_t(a) = \\hat{\\mu}_t (a) = \\dfrac{N_t^1(a)}{N_t^0(a)+N_t^1(a)}\\) (\\(N_t^r(a)\\) : the number of times (up to step \\(t-1\\))) that action \\(a\\) has been tried and the observed reward \\(r\\). <code>Chernoff-Heffding inequality</code>\ub97c \uc0ac\uc6a9\ud558\uba74 \\(\\delta_t (a) = c / \\sqrt{N_t(a)}\\) for some constant \\(c\\)\uc774\uace0, \uacb0\ub860\uc801\uc73c\ub85c \\(\\tilde{R}_t(a)=\\hat{\\mu}_t(a)+\\dfrac{c}{\\sqrt{N_t(a)}}\\) </p>"},{"location":"RL%20study/2024-02-19/#1433-bandit-case-bayesian-approach","title":"1.4.3.3 Bandit case: Bayesian approach","text":"<p>Bayesian inference\ub97c \ud1b5\ud574 upper confidence\ub97c \uad6c\ud560 \uc218\ub3c4 \uc788\ub2e4. </p> <p></p>"},{"location":"RL%20study/2024-02-19/#1434-mdp-case","title":"1.4.3.4 MDP case","text":"<p>frequentist form\uc758 UCB idea\ub294 MDP case\ub85c \ud655\uc7a5\ub420 \uc218 \uc788\ub294\ub370 [ACBF02]\uc5d0\uc11c\ub294 UCB\ub97c Q-learning\uacfc \ud63c\ud569\ud558\uc5ec \ub2e4\uc74c\uc758 policy\ub97c \uc815\uc758\ud55c\ub2e4.</p> \\[ \\pi(a|s) = \\mathbb{I} \\bigg( a=\\operatorname*{argmax}_{a'}\\Big[Q(s,a')+ c\\sqrt{\\log (t)/N_t(s,a') }\\Big]\\bigg) \\] <p>\uc774\uc678\uc5d0\ub3c4 <code>UCRL2</code> \uc640 \uac19\uc740 \ub354 \ubcf5\uc7a1\ud55c \ubc29\ubc95\ub3c4 \uc788\ub2e4.</p>"},{"location":"RL%20study/2024-02-19/#144-thompson-sampling","title":"1.4.4 Thompson sampling","text":"<p>UCB\uc758 \ub300\uccb4 \ubc29\ubc95\uc73c\ub85c <code>Thompson sampling</code>\uc774 \uc788\ub294\ub370 \uc774\ub294 <code>probability matching</code>\uc774\ub77c\uace0\ub3c4 \ubd88\ub9b0\ub2e4.</p>"},{"location":"RL%20study/2024-02-19/#1441-bandit-case","title":"1.4.4.1 Bandit case","text":"<p>\uc704 \uadf8\ub9bc\uc740 Thompson sampling\uc744 \uc774\uc6a9\ud55c linear regression bandit \uc608\uc2dc</p> <p>Thompson sampling quickly discovers that arm 1 is useless. Initially it pulls arm 2 more, but it adapts to the non-stationary nature of the problem and switches over to arm 0, as shown in Figure 1.6(b). In Figure 1.6(c), we show that the empirical cumulative regret in blue is close to the optimal lower bound in red.</p>"},{"location":"RL%20study/2024-02-19/#1442-mdp-case-posterior-sampling-rl","title":"1.4.4.2 MDP case (posterior sampling RL)","text":"<p>reward\uc640 transition model\uc5d0 \ub300\ud55c posterior\ub97c \uc720\uc9c0\ud55c \ucc44\ub85c sampling an MDP from this belief state at the start of each episode, solving for the optimal policy corresponding to the sampled MDP, using the resulting policy to collect new data, and then updating the belief state at the end of the episode. This is called <code>posterior sampling RL</code></p> <p>\ub610\ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub294 policy \ud639\uc740 Q-function\uc5d0 \ub300\ud55c posterior\ub97c \uc720\uc9c0\ud55c \ucc44\ub85c(world \ubaa8\ub378 \ub300\uc2e0) \ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub294\ub370, \uadf8 \uc911 \ud558\ub098\uc758 implementation\uc73c\ub85c <code>epistemic neural networks</code>\uac00 \uc788\ub2e4.</p> <p>\ub610\ub294 successor features\ub97c \ud65c\uc6a9\ud558\uae30\ub3c4 \ud558\uace0, <code>Successor Uncertainties</code>\ub77c\ub294 \uac1c\ub150\uc744 \uc0ac\uc6a9\ud558\uc5ec Q function\uc5d0 \ub300\ud574 posterior distribution\uc744 \uc720\ub3c4\ud558\uae30\ub3c4 \ud55c\ub2e4. </p>"},{"location":"RL%20study/2024-02-19/#2-value-based-rl","title":"2. Value-based RL","text":""},{"location":"RL%20study/2024-02-19/#23-computing-the-value-function-without-knowing-the-world-model","title":"2.3 Computing the value function without knowing the world model","text":"<p>\uc5ec\uae30\uc11c\ub294 \\((s',r)\\sim p(s',r|s,a)\\)\ub85c \uc811\uadfc\ud55c\ub2e4. world model\uc744 \ubaa8\ub974\uba74 estimate\uc744 \ub9ce\uc774 \ud574\uc57c\ub41c\ub2e4. \ubb3c\ub860 variance\uac00 \ud06c\ubbc0\ub85c \uac01\uc885 variance reduction \ub178\ub825\uc774 \ud544\uc694\ud558\ub2e4. (REINFORCE\uc758 base \uac19\uc774....)</p>"},{"location":"RL%20study/2024-02-19/#231-monte-carlo-estimation","title":"2.3.1 Monte Carlo estimation","text":""},{"location":"blog/","title":"Blog","text":""}]}