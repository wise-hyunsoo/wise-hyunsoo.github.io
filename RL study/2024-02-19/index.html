<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../2024-02-12/ rel=prev><link href=../../blog/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../feed_rss_updated.xml><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.0"><title>2024-02-19 - Blog for studying anything..</title><link rel=stylesheet href=../../assets/stylesheets/main.618322db.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#2024-02-19 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Blog for studying anything.." class="md-header__button md-logo" aria-label="Blog for studying anything.." data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Blog for studying anything.. </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2024-02-19 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Obsidian Notes </a> </li> <li class=md-tabs__item> <a href=../../Features/LaTeX%20Math%20Support/ class=md-tabs__link> Features </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../2024-02-12/ class=md-tabs__link> RL study </a> </li> <li class=md-tabs__item> <a href=../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Blog for studying anything.." class="md-nav__button md-logo" aria-label="Blog for studying anything.." data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Blog for studying anything.. </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Obsidian Notes </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Features </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Features/LaTeX%20Math%20Support/ class=md-nav__link> <span class=md-ellipsis> LaTeX Math Support </span> </a> </li> <li class=md-nav__item> <a href=../../Features/Mermaid%20Diagrams/ class=md-nav__link> <span class=md-ellipsis> Mermaid diagrams </span> </a> </li> <li class=md-nav__item> <a href=../../Features/Text%20Formatting/ class=md-nav__link> <span class=md-ellipsis> Text Formatting </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> RL study </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> RL study </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../2024-02-12/ class=md-nav__link> <span class=md-ellipsis> 2024-02-12 </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2024-02-19 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2024-02-19 </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#14-exploration-exploitation-tradeoff class=md-nav__link> <span class=md-ellipsis> 1.4 Exploration-exploitation tradeoff </span> </a> <nav class=md-nav aria-label="1.4 Exploration-exploitation tradeoff"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#141-simple-heuristics class=md-nav__link> <span class=md-ellipsis> 1.4.1 Simple heuristics </span> </a> </li> <li class=md-nav__item> <a href=#142-methods-based-on-the-belief-state-mdp class=md-nav__link> <span class=md-ellipsis> 1.4.2 Methods based on the belief state MDP </span> </a> <nav class=md-nav aria-label="1.4.2 Methods based on the belief state MDP"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1421-bandit-case-gittins-indices class=md-nav__link> <span class=md-ellipsis> 1.4.2.1 Bandit case (Gittins indices) </span> </a> </li> <li class=md-nav__item> <a href=#1422-mdp-case-bayes-adaptive-mdps class=md-nav__link> <span class=md-ellipsis> 1.4.2.2 MDP case (Bayes Adaptive MDPs) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#143-upper-confidence-bounds-ucbs class=md-nav__link> <span class=md-ellipsis> 1.4.3 Upper confidence bounds (UCBs) </span> </a> <nav class=md-nav aria-label="1.4.3 Upper confidence bounds (UCBs)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1431-basic-idea class=md-nav__link> <span class=md-ellipsis> 1.4.3.1 Basic idea </span> </a> </li> <li class=md-nav__item> <a href=#1432-bandit-case-frequentist-approach class=md-nav__link> <span class=md-ellipsis> 1.4.3.2 Bandit case: Frequentist approach </span> </a> </li> <li class=md-nav__item> <a href=#1433-bandit-case-bayesian-approach class=md-nav__link> <span class=md-ellipsis> 1.4.3.3 Bandit case: Bayesian approach </span> </a> </li> <li class=md-nav__item> <a href=#1434-mdp-case class=md-nav__link> <span class=md-ellipsis> 1.4.3.4 MDP case </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#144-thompson-sampling class=md-nav__link> <span class=md-ellipsis> 1.4.4 Thompson sampling </span> </a> <nav class=md-nav aria-label="1.4.4 Thompson sampling"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1441-bandit-case class=md-nav__link> <span class=md-ellipsis> 1.4.4.1 Bandit case </span> </a> </li> <li class=md-nav__item> <a href=#1442-mdp-case-posterior-sampling-rl class=md-nav__link> <span class=md-ellipsis> 1.4.4.2 MDP case (posterior sampling RL) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-value-based-rl class=md-nav__link> <span class=md-ellipsis> 2. Value-based RL </span> </a> <nav class=md-nav aria-label="2. Value-based RL"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#23-computing-the-value-function-without-knowing-the-world-model class=md-nav__link> <span class=md-ellipsis> 2.3 Computing the value function without knowing the world model </span> </a> <nav class=md-nav aria-label="2.3 Computing the value function without knowing the world model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-monte-carlo-estimation class=md-nav__link> <span class=md-ellipsis> 2.3.1 Monte Carlo estimation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> </a> </li> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Archive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#14-exploration-exploitation-tradeoff class=md-nav__link> <span class=md-ellipsis> 1.4 Exploration-exploitation tradeoff </span> </a> <nav class=md-nav aria-label="1.4 Exploration-exploitation tradeoff"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#141-simple-heuristics class=md-nav__link> <span class=md-ellipsis> 1.4.1 Simple heuristics </span> </a> </li> <li class=md-nav__item> <a href=#142-methods-based-on-the-belief-state-mdp class=md-nav__link> <span class=md-ellipsis> 1.4.2 Methods based on the belief state MDP </span> </a> <nav class=md-nav aria-label="1.4.2 Methods based on the belief state MDP"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1421-bandit-case-gittins-indices class=md-nav__link> <span class=md-ellipsis> 1.4.2.1 Bandit case (Gittins indices) </span> </a> </li> <li class=md-nav__item> <a href=#1422-mdp-case-bayes-adaptive-mdps class=md-nav__link> <span class=md-ellipsis> 1.4.2.2 MDP case (Bayes Adaptive MDPs) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#143-upper-confidence-bounds-ucbs class=md-nav__link> <span class=md-ellipsis> 1.4.3 Upper confidence bounds (UCBs) </span> </a> <nav class=md-nav aria-label="1.4.3 Upper confidence bounds (UCBs)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1431-basic-idea class=md-nav__link> <span class=md-ellipsis> 1.4.3.1 Basic idea </span> </a> </li> <li class=md-nav__item> <a href=#1432-bandit-case-frequentist-approach class=md-nav__link> <span class=md-ellipsis> 1.4.3.2 Bandit case: Frequentist approach </span> </a> </li> <li class=md-nav__item> <a href=#1433-bandit-case-bayesian-approach class=md-nav__link> <span class=md-ellipsis> 1.4.3.3 Bandit case: Bayesian approach </span> </a> </li> <li class=md-nav__item> <a href=#1434-mdp-case class=md-nav__link> <span class=md-ellipsis> 1.4.3.4 MDP case </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#144-thompson-sampling class=md-nav__link> <span class=md-ellipsis> 1.4.4 Thompson sampling </span> </a> <nav class=md-nav aria-label="1.4.4 Thompson sampling"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1441-bandit-case class=md-nav__link> <span class=md-ellipsis> 1.4.4.1 Bandit case </span> </a> </li> <li class=md-nav__item> <a href=#1442-mdp-case-posterior-sampling-rl class=md-nav__link> <span class=md-ellipsis> 1.4.4.2 MDP case (posterior sampling RL) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-value-based-rl class=md-nav__link> <span class=md-ellipsis> 2. Value-based RL </span> </a> <nav class=md-nav aria-label="2. Value-based RL"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#23-computing-the-value-function-without-knowing-the-world-model class=md-nav__link> <span class=md-ellipsis> 2.3 Computing the value function without knowing the world model </span> </a> <nav class=md-nav aria-label="2.3 Computing the value function without knowing the world model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-monte-carlo-estimation class=md-nav__link> <span class=md-ellipsis> 2.3.1 Monte Carlo estimation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=2024-02-19>2024-02-19<a class=headerlink href=#2024-02-19 title="Permanent link">&para;</a></h1> <h3 id=14-exploration-exploitation-tradeoff>1.4 Exploration-exploitation tradeoff<a class=headerlink href=#14-exploration-exploitation-tradeoff title="Permanent link">&para;</a></h3> <p>현재 당장 큰 reward를 위해 선택할 것인지 vs 더 넓은 state-action space를 활보함에 따라 구해질 수도 있는 더 큰 reward를 바랄 것인지</p> <h4 id=141-simple-heuristics>1.4.1 Simple heuristics<a class=headerlink href=#141-simple-heuristics title="Permanent link">&para;</a></h4> <ul> <li>Pure exploitation (greedy policy) <span class=arithmatex>\(a_t = \operatorname*{argmax}_a Q(s,a)\)</span> </li> <li><span class=arithmatex>\(\epsilon\)</span>-greedy policy <span class=arithmatex>\(\pi_\epsilon\)</span> parametrized by <span class=arithmatex>\(\epsilon \in [0,1]\)</span> <ul> <li><span class=arithmatex>\(1-\epsilon\)</span>의 확률로 <span class=arithmatex>\(a_t=\operatorname*{argmax}_a \hat{R}_t(s_t,a)\)</span></li> <li><span class=arithmatex>\(\epsilon\)</span>의 확률로 random action</li> <li>이 방법은 agent가 all state-action 조합에 대해 계속 exploration을 하게 해 줌.</li> <li>그러나 이 방법론은 <code>suboptimal</code>이다. 왜냐하면 <span class=arithmatex>\(\epsilon/ |\mathcal{A}|\)</span>의 확률로 random action을 고르기 때문이다.<ul> <li>따라서 <span class=arithmatex>\(\epsilon\)</span>을 시간에 따라 0으로 annealing하는 방법을 통해 이러한 suboptimal 현상을 완화할 수 있다.</li> </ul> </li> <li>이 방법의 또 다른 문제점은 <code>dithering</code>이라는 것인데, 이는 agent가 계속 해서 무엇을 할지 마음을 바꾼다는 의미이다.<ul> <li>이걸 해결하기 위해서 <span class=arithmatex>\(\epsilon z\)</span>-greedy 방법론(<a href=https://arxiv.org/abs/2006.01782>Temporally-Extended ε-Greedy Exploration</a>)이 제시되었는데 방법은 다음과 같다. (FYI: 발음은 easy-greedy)<ul> <li><span class=arithmatex>\(\epsilon\)</span>-greedy와 마찬가지로 <span class=arithmatex>\(1-\epsilon\)</span>의 확률로 exploitation을 한다.</li> <li><span class=arithmatex>\(\epsilon\)</span>의 확률로는 <span class=arithmatex>\(n\)</span>번 같은 action을 반복한다.</li> <li><span class=arithmatex>\(n\)</span>은 exponential, uniform, zeta distribution 중에서 sampling된다. (논문에서는 zeta distribution with <span class=arithmatex>\(\mu=2\)</span>가 일관성있게 좋은 결과가 나온다고 실험적으로 보였다.)</li> <li>이 방법론의 메인 가설 혹은 주장은 <span class=arithmatex>\(\epsilon\)</span>-greedy의 주된 단점이 temporal persistence가 부족하다는 것이다. 이렇게 같은 action으로 쭉 같은 행동을 어느정도 해줘야 local optima를 탈출할 수 있다고 주장한다. </li> <li>또한 이 방법들은 야생의 동물 세계에서 동물들이 먹이 채집을 하러 방향을 정할 때 랜덤하게 하나의 방향을 정해서 어느정도 heavy-tailed distribution을 따르는 duration만큼 계속 전진하는 ecological 모델이 optimal 동물 채집 전략이라는 사실이라는 것이 알려졌기에 이에 motivate된 방법론이라 할 수 있다.</li> </ul> </li> </ul> </li> <li><span class=arithmatex>\(\epsilon\)</span>-greedy가 아닌 또 다른 방법으로 <code>Boltzmann exploration</code>이라는 것이 있다. 메인 아이디어는 당장의 reward가 높은 action에 높은 확률을 부여하는 것이다.<ul> <li>이 방법론은 reward estimate의 결과에 따라 policy의 변화가 좀더 smoother해지는 것이다.</li> </ul> </li> </ul> </li> </ul> <div class=arithmatex>\[ \pi_\tau(a|s) = \dfrac{\exp (\hat{R}_t(s_t,a)/\tau)}{\sum_{a'}\exp (\hat{R}_t (s_t,a')/\tau)} \]</div> <p><span class=arithmatex>\(\tau\)</span>가 0으로 가면 greedy policy가 가까워지고, <span class=arithmatex>\(\tau\)</span>가 무한대로 가면 uniform에 가까워진다. (아래 그림은 오타가 있는 듯 함)</p> <p><img alt="Pasted image 20250217132345.png" src=../attachments/Pasted%20image%2020250217132345.png></p> <ul> <li>마지막으로 또다른 접근 법으로는 outcome의 결과(consequences of the outcome)가 uncertain한 (state, action)을 더 explore하도록 하는 방법론이 있다.<ul> <li>이 방법은 <code>exploration bonus</code> <span class=arithmatex>\(R_t^b(s,a)\)</span>를 통해 이루어지는데, 이는 state s일 때 action a를 실시한 횟수가 적을 때 큰 값을 부여하도록 bonus가 설계된다.</li> <li>이 bonus를 원래의 reward에 더 하여 사용하는데, 이렇게 하여 agent가 world에 대해 더 탐색하여 useful information을 얻도록 하는 효과가 있다. 이러한 reward를 <code>intrinsic reward</code> 함수라고 부른다. (Section 5.2.4에서 더 자세히 다루는듯)</li> </ul> </li> </ul> <h4 id=142-methods-based-on-the-belief-state-mdp>1.4.2 Methods based on the belief state MDP<a class=headerlink href=#142-methods-based-on-the-belief-state-mdp title="Permanent link">&para;</a></h4> <p>Bayesian approach를 적용하여 exploration-exploitation tradeoff에 대한 optimal solution을 계산할 수 있는데 그에 대해 다루는 section이다.</p> <h5 id=1421-bandit-case-gittins-indices>1.4.2.1 Bandit case (Gittins indices)<a class=headerlink href=#1421-bandit-case-gittins-indices title="Permanent link">&para;</a></h5> <p>모델 파라메터에 대한 belief state, <span class=arithmatex>\(p(\theta_t|\mathcal{D}_{1:t})\)</span>를 recursive하게 계산할 수 있다고 가정하자. 이 상황의 belief state MDP에서 우리는 policy를 어떻게 계산하는가(구하는가)?</p> <p>context-free bandits with a finite number of arms인 경우에는 optimal policy를 dynamic programming을 통해 구할 수 있다.</p> <ul> <li>그 결과는 매 스텝별 table of action probabilities <span class=arithmatex>\(\pi_t(a_1,\cdots,a_K)\)</span> 으로 표현될 수 있는데, 이를 <code>Gittins indices</code>라 부른다.</li> <li>물론 general contextual bandits에 대한 optimal policy는 intractable하다.</li> </ul> <p><img alt="Pasted image 20250217134705.png" src=../attachments/Pasted%20image%2020250217134705.png></p> <h5 id=1422-mdp-case-bayes-adaptive-mdps>1.4.2.2 MDP case (Bayes Adaptive MDPs)<a class=headerlink href=#1422-mdp-case-bayes-adaptive-mdps title="Permanent link">&para;</a></h5> <p>위 방법론을 MDP case에 적용하여 BAMDP(Bayes Adaptive MDP)로 바꿔서 적용할 수 있는데, 이러한 확장적용 방법론이 computationally intractable to solve한 것이 알려져 있어서 다양한 근사 방법으로 BADMP를 푼다고 한다.</p> <h4 id=143-upper-confidence-bounds-ucbs>1.4.3 Upper confidence bounds (UCBs)<a class=headerlink href=#143-upper-confidence-bounds-ucbs title="Permanent link">&para;</a></h4> <p>explore-exploit에 대한 optimal solution은 intractable하다. 하지만 직관적인 방법(원리)에 기초한 방법론이 사용될 수 있는데, 그 원리는 <code>optimism in the face of uncertainty (OFU)</code> 라는 것이다. 이 원리는 action을 greedy하게 선택하는데, 그 선택의 근거가 optimistic estimate of reward로 인한 선택이 되도록 하는 것이다. 이 방법론이 optimal하다는 것은 <code>R-max</code> paper에 증명되어 있는데, 이 증명은 선행 연구였던 <code>E3</code> paper의 결과에 기초하여 증명되었다.</p> <p>이 원리의 가장 일반적인 implementation은 <code>upper confidence bound (UCB)</code>라는 개념에 근거하여 이루어지는데, 우리는 bandit case에서 이를 설명하고, 그 다음에 MDP case에 대해 다루겠다.</p> <h5 id=1431-basic-idea>1.4.3.1 Basic idea<a class=headerlink href=#1431-basic-idea title="Permanent link">&para;</a></h5> <ul> <li>UCB 방법을 쓰기 위해서는 agent가 optimistic reward function을 사용한다고 가정한다. 즉, 높은 확률로 <span class=arithmatex>\(\tilde{R}_t(s_t,a) \geq R(s_t,a) \ (\forall a\in \mathcal{A})\)</span> 인 상황에서 다음과 같은 greedy action을 취한다.</li> </ul> <div class=arithmatex>\[ a_t = \operatorname*{argmax}_a \tilde{R}_t (s_t,a) \]</div> <ul> <li>UCB는 optimistic estimate가 exploration을 장려하는 <code>exploration bonus</code>의 형태로 볼 수 있다.</li> <li>일반적으로 the amount of optimism, <span class=arithmatex>\(\tilde{R}_t-R\)</span>이 시간에 따라 감소하며, 따라서 agent는 exploration을 줄이게 된다.</li> <li>적절하게 설계된 optimistic reward estimate <span class=arithmatex>\(\tilde{R}\)</span>을 통해 UCB 방법론이 많은 variants of bandits에서 near-optimal regret을 가진다는 것이 알려져 있다.<ul> <li>optimistic function <span class=arithmatex>\(\tilde{R}\)</span>은 다양한 방법으로 설계될 수 있는데 다음 section들에서 다룬다.</li> </ul> </li> </ul> <h5 id=1432-bandit-case-frequentist-approach>1.4.3.2 Bandit case: Frequentist approach<a class=headerlink href=#1432-bandit-case-frequentist-approach title="Permanent link">&para;</a></h5> <ul> <li>Frequentist approach에서는 confidence bound를 계산할 때 estimation error의 높은 확률의 상한(upper bound)을 유도하기 위해 <code>concentratioin inequality</code> 를 사용한다.</li> </ul> <div class=arithmatex>\[ | \hat{R}_t (s,a)-R_t(s,a)| \leq \delta_t (s,a) \]</div> <ul> <li><span class=arithmatex>\(\hat{R}_t\)</span> : usua estimate of <span class=arithmatex>\(R\)</span> (often the MLE), <span class=arithmatex>\(\delta_t\)</span> : properly selected function</li> <li>Optimistic reward <span class=arithmatex>\(\tilde{R}_t (s,a)= \hat{R}_t(s,a) +\delta_t(s,a)\)</span> 로 설정한다.</li> </ul> <div class="admonition examples"> <p class=admonition-title>Examples</p> <p>context-free Bernoulli bandit, <span class=arithmatex>\(R(a)\sim \text{Ber}(\mu(a))\)</span> MLE <span class=arithmatex>\(\hat{R}_t(a) = \hat{\mu}_t (a) = \dfrac{N_t^1(a)}{N_t^0(a)+N_t^1(a)}\)</span> (<span class=arithmatex>\(N_t^r(a)\)</span> : the number of times (up to step <span class=arithmatex>\(t-1\)</span>)) that action <span class=arithmatex>\(a\)</span> has been tried and the observed reward <span class=arithmatex>\(r\)</span>. <code>Chernoff-Heffding inequality</code>를 사용하면 <span class=arithmatex>\(\delta_t (a) = c / \sqrt{N_t(a)}\)</span> for some constant <span class=arithmatex>\(c\)</span>이고, 결론적으로 <span class=arithmatex>\(\tilde{R}_t(a)=\hat{\mu}_t(a)+\dfrac{c}{\sqrt{N_t(a)}}\)</span> </p> </div> <h5 id=1433-bandit-case-bayesian-approach>1.4.3.3 Bandit case: Bayesian approach<a class=headerlink href=#1433-bandit-case-bayesian-approach title="Permanent link">&para;</a></h5> <p>Bayesian inference를 통해 upper confidence를 구할 수도 있다. </p> <p><img alt="Pasted image 20250217140814.png" src=../attachments/Pasted%20image%2020250217140814.png></p> <h5 id=1434-mdp-case>1.4.3.4 MDP case<a class=headerlink href=#1434-mdp-case title="Permanent link">&para;</a></h5> <p>frequentist form의 UCB idea는 MDP case로 확장될 수 있는데 [ACBF02]에서는 UCB를 Q-learning과 혼합하여 다음의 policy를 정의한다.</p> <div class=arithmatex>\[ \pi(a|s) = \mathbb{I} \bigg( a=\operatorname*{argmax}_{a'}\Big[Q(s,a')+ c\sqrt{\log (t)/N_t(s,a') }\Big]\bigg) \]</div> <p>이외에도 <code>UCRL2</code> 와 같은 더 복잡한 방법도 있다.</p> <h4 id=144-thompson-sampling>1.4.4 Thompson sampling<a class=headerlink href=#144-thompson-sampling title="Permanent link">&para;</a></h4> <p>UCB의 대체 방법으로 <code>Thompson sampling</code>이 있는데 이는 <code>probability matching</code>이라고도 불린다.</p> <h5 id=1441-bandit-case>1.4.4.1 Bandit case<a class=headerlink href=#1441-bandit-case title="Permanent link">&para;</a></h5> <p><img alt="Pasted image 20250217160148.png" src=../attachments/Pasted%20image%2020250217160148.png></p> <p><img alt="Pasted image 20250214143340.png" src=../attachments/Pasted%20image%2020250214143340.png></p> <p>위 그림은 Thompson sampling을 이용한 linear regression bandit 예시</p> <p>Thompson sampling quickly discovers that arm 1 is useless. Initially it pulls arm 2 more, but it adapts to the non-stationary nature of the problem and switches over to arm 0, as shown in Figure 1.6(b). In Figure 1.6(c), we show that the empirical cumulative regret in blue is close to the optimal lower bound in red.</p> <h5 id=1442-mdp-case-posterior-sampling-rl>1.4.4.2 MDP case (posterior sampling RL)<a class=headerlink href=#1442-mdp-case-posterior-sampling-rl title="Permanent link">&para;</a></h5> <p>reward와 transition model에 대한 posterior를 유지한 채로 sampling an MDP from this belief state at the start of each episode, solving for the optimal policy corresponding to the sampled MDP, using the resulting policy to collect new data, and then updating the belief state at the end of the episode. This is called <code>posterior sampling RL</code></p> <p>또다른 방법으로는 policy 혹은 Q-function에 대한 posterior를 유지한 채로(world 모델 대신) 하는 방법이 있는데, 그 중 하나의 implementation으로 <code>epistemic neural networks</code>가 있다.</p> <p>또는 successor features를 활용하기도 하고, <code>Successor Uncertainties</code>라는 개념을 사용하여 Q function에 대해 posterior distribution을 유도하기도 한다. <img alt="Pasted image 20250217161513.png" src=../attachments/Pasted%20image%2020250217161513.png></p> <h2 id=2-value-based-rl>2. Value-based RL<a class=headerlink href=#2-value-based-rl title="Permanent link">&para;</a></h2> <h3 id=23-computing-the-value-function-without-knowing-the-world-model>2.3 Computing the value function without knowing the world model<a class=headerlink href=#23-computing-the-value-function-without-knowing-the-world-model title="Permanent link">&para;</a></h3> <p>여기서는 <span class=arithmatex>\((s',r)\sim p(s',r|s,a)\)</span>로 접근한다. world model을 모르면 estimate을 많이 해야된다. 물론 variance가 크므로 각종 variance reduction 노력이 필요하다. (REINFORCE의 base 같이....)</p> <h4 id=231-monte-carlo-estimation>2.3.1 Monte Carlo estimation<a class=headerlink href=#231-monte-carlo-estimation title="Permanent link">&para;</a></h4> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../2024-02-12/ class="md-footer__link md-footer__link--prev" aria-label="Previous: 2024-02-12"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 2024-02-12 </div> </div> </a> <a href=../../blog/ class="md-footer__link md-footer__link--next" aria-label="Next: Blog"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Blog </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.footer"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.e71a0d61.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://unpkg.com/mermaid/dist/mermaid.min.js></script> </body> </html>