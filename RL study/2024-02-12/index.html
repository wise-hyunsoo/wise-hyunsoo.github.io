<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../../Features/Text%20Formatting/ rel=prev><link href=../2024-02-19/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../feed_rss_updated.xml><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.22"><title>2024-02-12 - Blog for studying anything..</title><link rel=stylesheet href=../../assets/stylesheets/main.84d31ad4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#2024-02-12 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Blog for studying anything.." class="md-header__button md-logo" aria-label="Blog for studying anything.." data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Blog for studying anything.. </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2024-02-12 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Obsidian Notes </a> </li> <li class=md-tabs__item> <a href=../../Features/LaTeX%20Math%20Support/ class=md-tabs__link> Features </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> RL study </a> </li> <li class=md-tabs__item> <a href=../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Blog for studying anything.." class="md-nav__button md-logo" aria-label="Blog for studying anything.." data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Blog for studying anything.. </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Obsidian Notes </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Features </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Features/LaTeX%20Math%20Support/ class=md-nav__link> <span class=md-ellipsis> LaTeX Math Support </span> </a> </li> <li class=md-nav__item> <a href=../../Features/Mermaid%20Diagrams/ class=md-nav__link> <span class=md-ellipsis> Mermaid diagrams </span> </a> </li> <li class=md-nav__item> <a href=../../Features/Text%20Formatting/ class=md-nav__link> <span class=md-ellipsis> Text Formatting </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> RL study </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> RL study </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2024-02-12 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2024-02-12 </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-introduction class=md-nav__link> <span class=md-ellipsis> 1. Introduction </span> </a> <nav class=md-nav aria-label="1. Introduction"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-sequential-decision-making class=md-nav__link> <span class=md-ellipsis> 1.1 Sequential decision making </span> </a> <nav class=md-nav aria-label="1.1 Sequential decision making"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#111-problem-definition class=md-nav__link> <span class=md-ellipsis> 1.1.1 Problem definition </span> </a> </li> <li class=md-nav__item> <a href=#112-universal-model class=md-nav__link> <span class=md-ellipsis> 1.1.2 Universal model </span> </a> </li> <li class=md-nav__item> <a href=#113-episodic-vs-continuing-task class=md-nav__link> <span class=md-ellipsis> 1.1.3 Episodic vs continuing task </span> </a> </li> <li class=md-nav__item> <a href=#114-regret class=md-nav__link> <span class=md-ellipsis> 1.1.4 Regret </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#12-canonical-examples class=md-nav__link> <span class=md-ellipsis> 1.2 Canonical examples </span> </a> <nav class=md-nav aria-label="1.2 Canonical examples"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#121-partially-observed-mdps-pomdp class=md-nav__link> <span class=md-ellipsis> 1.2.1 Partially observed MDPs (POMDP) </span> </a> </li> <li class=md-nav__item> <a href=#122-markov-decision-process-mdps class=md-nav__link> <span class=md-ellipsis> 1.2.2 Markov decision process (MDPs) </span> </a> </li> <li class=md-nav__item> <a href=#123-contextual-mdps class=md-nav__link> <span class=md-ellipsis> 1.2.3 Contextual MDPs </span> </a> </li> <li class=md-nav__item> <a href=#124-contextual-bandits class=md-nav__link> <span class=md-ellipsis> 1.2.4 Contextual bandits </span> </a> </li> <li class=md-nav__item> <a href=#125-belief-state-mdps class=md-nav__link> <span class=md-ellipsis> 1.2.5 Belief state MDPs </span> </a> </li> <li class=md-nav__item> <a href=#126-optimization-problems class=md-nav__link> <span class=md-ellipsis> 1.2.6 Optimization problems </span> </a> <nav class=md-nav aria-label="1.2.6 Optimization problems"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1261-best-arm-identification class=md-nav__link> <span class=md-ellipsis> 1.2.6.1 Best-arm identification </span> </a> </li> <li class=md-nav__item> <a href=#1262-bayesian-optimization class=md-nav__link> <span class=md-ellipsis> 1.2.6.2 Bayesian optimization </span> </a> </li> <li class=md-nav__item> <a href=#1263-active-learning class=md-nav__link> <span class=md-ellipsis> 1.2.6.3 Active learning </span> </a> </li> <li class=md-nav__item> <a href=#1264-stochastic-gradient-descent-sgd class=md-nav__link> <span class=md-ellipsis> 1.2.6.4 Stochastic Gradient Descent (SGD) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#134-dealing-with-partial-observability class=md-nav__link> <span class=md-ellipsis> 1.3.4 Dealing with partial observability </span> </a> <nav class=md-nav aria-label="1.3.4 Dealing with partial observability"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1341-optimal-solution class=md-nav__link> <span class=md-ellipsis> 1.3.4.1 Optimal solution </span> </a> </li> <li class=md-nav__item> <a href=#1342-finite-observation-history class=md-nav__link> <span class=md-ellipsis> 1.3.4.2 Finite observation history </span> </a> </li> <li class=md-nav__item> <a href=#1343-stateful-recurrent-policies class=md-nav__link> <span class=md-ellipsis> 1.3.4.3 Stateful (recurrent) policies </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../2024-02-19/ class=md-nav__link> <span class=md-ellipsis> 2024-02-19 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> </a> </li> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Archive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-introduction class=md-nav__link> <span class=md-ellipsis> 1. Introduction </span> </a> <nav class=md-nav aria-label="1. Introduction"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-sequential-decision-making class=md-nav__link> <span class=md-ellipsis> 1.1 Sequential decision making </span> </a> <nav class=md-nav aria-label="1.1 Sequential decision making"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#111-problem-definition class=md-nav__link> <span class=md-ellipsis> 1.1.1 Problem definition </span> </a> </li> <li class=md-nav__item> <a href=#112-universal-model class=md-nav__link> <span class=md-ellipsis> 1.1.2 Universal model </span> </a> </li> <li class=md-nav__item> <a href=#113-episodic-vs-continuing-task class=md-nav__link> <span class=md-ellipsis> 1.1.3 Episodic vs continuing task </span> </a> </li> <li class=md-nav__item> <a href=#114-regret class=md-nav__link> <span class=md-ellipsis> 1.1.4 Regret </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#12-canonical-examples class=md-nav__link> <span class=md-ellipsis> 1.2 Canonical examples </span> </a> <nav class=md-nav aria-label="1.2 Canonical examples"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#121-partially-observed-mdps-pomdp class=md-nav__link> <span class=md-ellipsis> 1.2.1 Partially observed MDPs (POMDP) </span> </a> </li> <li class=md-nav__item> <a href=#122-markov-decision-process-mdps class=md-nav__link> <span class=md-ellipsis> 1.2.2 Markov decision process (MDPs) </span> </a> </li> <li class=md-nav__item> <a href=#123-contextual-mdps class=md-nav__link> <span class=md-ellipsis> 1.2.3 Contextual MDPs </span> </a> </li> <li class=md-nav__item> <a href=#124-contextual-bandits class=md-nav__link> <span class=md-ellipsis> 1.2.4 Contextual bandits </span> </a> </li> <li class=md-nav__item> <a href=#125-belief-state-mdps class=md-nav__link> <span class=md-ellipsis> 1.2.5 Belief state MDPs </span> </a> </li> <li class=md-nav__item> <a href=#126-optimization-problems class=md-nav__link> <span class=md-ellipsis> 1.2.6 Optimization problems </span> </a> <nav class=md-nav aria-label="1.2.6 Optimization problems"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1261-best-arm-identification class=md-nav__link> <span class=md-ellipsis> 1.2.6.1 Best-arm identification </span> </a> </li> <li class=md-nav__item> <a href=#1262-bayesian-optimization class=md-nav__link> <span class=md-ellipsis> 1.2.6.2 Bayesian optimization </span> </a> </li> <li class=md-nav__item> <a href=#1263-active-learning class=md-nav__link> <span class=md-ellipsis> 1.2.6.3 Active learning </span> </a> </li> <li class=md-nav__item> <a href=#1264-stochastic-gradient-descent-sgd class=md-nav__link> <span class=md-ellipsis> 1.2.6.4 Stochastic Gradient Descent (SGD) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#134-dealing-with-partial-observability class=md-nav__link> <span class=md-ellipsis> 1.3.4 Dealing with partial observability </span> </a> <nav class=md-nav aria-label="1.3.4 Dealing with partial observability"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1341-optimal-solution class=md-nav__link> <span class=md-ellipsis> 1.3.4.1 Optimal solution </span> </a> </li> <li class=md-nav__item> <a href=#1342-finite-observation-history class=md-nav__link> <span class=md-ellipsis> 1.3.4.2 Finite observation history </span> </a> </li> <li class=md-nav__item> <a href=#1343-stateful-recurrent-policies class=md-nav__link> <span class=md-ellipsis> 1.3.4.3 Stateful (recurrent) policies </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=2024-02-12>2024-02-12<a class=headerlink href=#2024-02-12 title="Permanent link">&para;</a></h1> <h2 id=1-introduction>1. Introduction<a class=headerlink href=#1-introduction title="Permanent link">&para;</a></h2> <h3 id=11-sequential-decision-making>1.1 Sequential decision making<a class=headerlink href=#11-sequential-decision-making title="Permanent link">&para;</a></h3> <h4 id=111-problem-definition>1.1.1 Problem definition<a class=headerlink href=#111-problem-definition title="Permanent link">&para;</a></h4> <div class=arithmatex>\[ V_\pi (s_0) = \mathbb{E}_{p(a_0,s_1,a_1,\cdots,a_T,s_T|s_0,\pi)} \bigg[ \sum_{t=0}^T R(s_t,a_t)|s_0 \bigg] \]</div> <div class=arithmatex>\[\begin{align*} p(a_0,s_1,a_1,\cdots,a_T,s_T|s_0,\pi) &amp;= \pi(a_0|s_0)p_{\text{env}}(o_1|a_0)\delta(s_1=U(s_0,a_0,o_1)) \\ &amp;\times \pi(a_1|s_1) p_\text{env}(o_2|a_1,o_1)\delta(s_2=U(s_1,a_1,o_2)) \\ &amp; \times \pi(a_2|s_2)p_\text{env}(o_3|a_{1:2},o_{1:2})\delta(s_3=U(s_2,a_2,o_3))\cdots \end{align*}\]</div> <ul> <li>Remark</li> </ul> <div class=arithmatex>\[ p_{S}(s_t|s_{t-1},a_{t-1})=\sum_{\lbrace o_t|s_t=U(s_{t-1},a_{t-1},o_t)\rbrace} p_{\text{env}}(o_t | a_{1:t-1}, o_{1:t-1}) \]</div> <ul> <li>Maximum expected utility (principle) and Optimal policy</li> </ul> <div class=arithmatex>\[ \pi^* = \operatorname*{argmax}_\pi \mathbb{E}_{p_0(s_0)} [ V_\pi(s_0)] \]</div> <h4 id=112-universal-model>1.1.2 Universal model<a class=headerlink href=#112-universal-model title="Permanent link">&para;</a></h4> <p><img alt="Pasted image 20250211103219.png" src=../attachments/Pasted%20image%2020250211103219.png></p> <ul> <li> <p>hidden state <span class=arithmatex>\(z_t\)</span> 혹은 world state</p> <ul> <li>update는 non-determistic하게 <span class=arithmatex>\(z_{t+1}=W(z_t,a_t,\epsilon_t^z)\)</span> </li> </ul> </li> <li> <p>agent는 potentially noisy 혹은 partial observation <span class=arithmatex>\(o_{t+1}=O(z_{t+1},\epsilon_{t+1}^o)\)</span> 를 보게 됨</p> <ul> <li><span class=arithmatex>\(o_t\)</span>는 <em>perceptual aliasing</em>이 있을 수 있음</li> </ul> </li> <li> <p>agent의 . tate <span class=arithmatex>\(s_t\)</span> 는 environment의 믿음(추측)을 담고 있음. 따라서 belief state 라고 부르기도 함.</p> <ul> <li>내부의 믿음 state가 있기에 perceptual aliasing이 있는 상황에서 외부상황에 대한 추측으로 같은 observation <span class=arithmatex>\(o\)</span>가 관측됐다 하더라도 내부 믿음 상황(agent의 내부 state <span class=arithmatex>\(s_t\)</span>)에 따라 다른 판단(action)을 내릴 수 있게 됨.</li> <li><span class=arithmatex>\(s_{t+1}=SU(s_t,a_t,o_{t+1})=U(s_{t+1|t},o_{t+1})=U(P(s_t,a_t),o_{t+1})\)</span> 와 같은 식으로 내부 믿음이 update됨.<ul> <li><span class=arithmatex>\(SU\)</span>: state update function, <span class=arithmatex>\(P\)</span>: prediction function, <span class=arithmatex>\(U\)</span>: update function</li> </ul> </li> </ul> </li> <li> <p>RL 방법론에 따라 agent가 외부 observation을 예측하려는 시도도 함</p> <ul> <li><span class=arithmatex>\(\hat{o}_{t+1}=D(s_{t+1|t})\)</span> 로 다음 observation을 추측하고 이를 decoder <span class=arithmatex>\(D\)</span>로 모델링하고 이를 훈련에 활용하기도 한다고 함(아마도 Model-based RL에서 하는듯)</li> <li>참고로 model-based RL에서 모델은 외부환경 혹은 World에 대한 modeling을 말하는 듯<ul> <li>그중 state transition or dynamics model <span class=arithmatex>\(p_S(s'|s,a)\)</span> 를 학습하는 방법론이 있는 듯</li> </ul> </li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Note</p> <p><a href=https://arxiv.org/abs/2310.01706>On Representation Complexity of Model-based and Model-free Reinforcement Learning</a> 에서는 이러한 dynamics를 학습하는 듯. 논문에 따르면 <code>The sample complexity of learning the dynamics is less than the sample complexity of learning the policy.</code> 이라 한다.</p> </div> <h4 id=113-episodic-vs-continuing-task>1.1.3 Episodic vs continuing task<a class=headerlink href=#113-episodic-vs-continuing-task title="Permanent link">&para;</a></h4> <dl> <dt><code>Continuing task</code></dt> <dd>끝이 agent와 environment가 끝없이 상호작용</dd> <dt><code>Episodic task</code></dt> <dd>언젠가는 끝남. 끝나는 시간이 정해져 있으면 finite horizon problem.</dd> </dl> <p><img alt="Pasted image 20250212191708.png" src=../attachments/Pasted%20image%2020250212191708.png></p> <ul> <li>reward-to-go <span class=arithmatex>\(G_t\)</span> 는 random variable. </li> <li> <p>그 random variable의 <span class=arithmatex>\(\pi\)</span>에 대한 기댓값(정확히는 <span class=arithmatex>\(s_t\)</span>에 대한 조건부 기댓값)으로 value function <span class=arithmatex>\(V_\pi (s_t)\)</span> 를 정의</p> </li> <li> <p>Discount factor <span class=arithmatex>\(\gamma \in [0,1]\)</span></p> <ul> <li><span class=arithmatex>\(\gamma \in [0,1)\)</span>를 쓰는 이유로 첫번째는 reward가 bounded일 때, value function의 수렴성 보장을 위해 사용, 두번째로는 short-term reward에 가중치를 주려고 사용, 즉 <span class=arithmatex>\(\gamma =1\)</span>인 경우는 현재랑 가까운 reward나 미래에 받는 reward나 그 값이 같으면 똑같다고 생각.</li> <li><span class=arithmatex>\(\gamma\)</span>가 작을 수록 근시안적(myopic)이 됨. 따라서 <span class=arithmatex>\(1-\gamma\)</span>는 다음 step에서 종료될 기대(혹은 확률)로 해석가능</li> </ul> </li> </ul> <h4 id=114-regret>1.1.4 Regret<a class=headerlink href=#114-regret title="Permanent link">&para;</a></h4> <ul> <li>per-step regret at <span class=arithmatex>\(t\)</span> : <span class=arithmatex>\(l_t\)</span> <ul> <li>oracle policy를 알아야 써먹을 수 있을듯</li> </ul> </li> </ul> <div class=arithmatex>\[ l_t := \mathbb{E}_{s_{1:t}} \big[R(s_t,\pi_*(s_t))-\mathbb{E}_{\pi(a_t|s_t)}[R(s_t,a_t)]\big] \]</div> <ul> <li> <p>simple regret at the last step, <span class=arithmatex>\(l_T\)</span> </p> <ul> <li>simple regret을 최적화하는 문제를 pure exploration이라 부름</li> </ul> </li> <li> <p>cumulative regret (or total regret or just the regret) <span class=arithmatex>\(L_T\)</span> </p> <ul> <li>이걸로 축적하면서 모델이랑 policy를 학습하면 <code>earning while learning</code> 이라 하는듯</li> <li>exploration-exploitation tradeoff를 해결하면서 문제를 풀어야 함</li> </ul> </li> </ul> <div class=arithmatex>\[ L_T := \mathbb{E}\bigg[ \sum_{t=1}^T l_t \bigg] \]</div> <h3 id=12-canonical-examples>1.2 Canonical examples<a class=headerlink href=#12-canonical-examples title="Permanent link">&para;</a></h3> <h4 id=121-partially-observed-mdps-pomdp>1.2.1 Partially observed MDPs (POMDP)<a class=headerlink href=#121-partially-observed-mdps-pomdp title="Permanent link">&para;</a></h4> <ul> <li>Figure 1.2의 그림이 POMDP 모델이라 함</li> <li>environment dynamic model 을 아래와 같이 stochastic transition function <span class=arithmatex>\(W\)</span> 으로 </li> </ul> <div class=arithmatex>\[ p(z_{t+1}|z_t, a_t) = \mathbb{E}_{\epsilon_t^z}[\mathbb{I}(z_{t+1}=W(z_t,a_t,\epsilon_t^z))] \]</div> <ul> <li>stochastic observation function <span class=arithmatex>\(O\)</span> 은 </li> </ul> <div class=arithmatex>\[ p(o_{t+1}|z_{t+1}) = \mathbb{E}_{\epsilon_{t+1}^o} [\mathbb{I}(o_{t+1}=O(z_{t+1},\epsilon_{t+1}^o))] \]</div> <p>와 같이 사용하여 <code>joint world model</code> <span class=arithmatex>\(p_{\text{WO}}(z_{t+1},o_{t+1}|z_t,a_t)\)</span> 을 도출</p> <p>또는 non-Markovian observation distribution <span class=arithmatex>\(p_{\text{env}} (o_{t+1}|o_{1:t}, a_{1:t})\)</span> 를 아래와 같이 정의 <img alt="Pasted image 20250212194031.png" src=../attachments/Pasted%20image%2020250212194031.png></p> <ul> <li>두 개의 transition distribution <span class=arithmatex>\(p(o|z), p(z'|z,a)\)</span> 가 world model의 정보<ul> <li>두 분포를 안다면(world model 을 안다면) 원리적으로는 optimal policy를 구할 수 있다고 한다.</li> <li>agent의 internal state <span class=arithmatex>\(s_t\)</span> 혹은 belief state <span class=arithmatex>\(b_t\)</span>를 <span class=arithmatex>\(s_t=b_t=p(z_t|h_t), \ h_t=(o_{1:t}, a_{1:t-1})\)</span> 로 두면 구할 수 있다고 함</li> <li>belief state는 원리적으로(통계적으로) optimal policy에 대해 충분통계량이라고 함.<ul> <li>데이터로 부터 belief state를 계산하면 그 belief state가 policy의 parameter와 독립적이라는 뜻</li> <li>그러나 computing belief state is wildly intractible 이라 함.</li> </ul> </li> </ul> </li> </ul> <h4 id=122-markov-decision-process-mdps>1.2.2 Markov decision process (MDPs)<a class=headerlink href=#122-markov-decision-process-mdps title="Permanent link">&para;</a></h4> <ul> <li> <p>MDP는 special case of POMDP</p> <ul> <li><span class=arithmatex>\(z_t=o_t=s_t\)</span> 인 상황을 가정</li> <li>따라서 <code>state transition matrix induce by the world model</code> : <span class=arithmatex>\(p_S(s_{t+1}|s_t,a_t):=\mathbb{E}_{\epsilon_t^s}[\mathbb{I} (s_{t+1}=W(s_t,a_t,\epsilon_t^s))]\)</span> 로 정의하여 MDP를 기술</li> </ul> </li> <li> <p>POMDP 에서 사용되던 observation 관점에서 MDP에서는 observation으로 reward를 environment에게서 받는다고 생각한다.</p> <ul> <li>expected reward(사실 조건부 기댓값)는 두가지 형태가 있고 다음과 같이 정의 <img alt="Pasted image 20250212195751.png" src=../attachments/Pasted%20image%2020250212195751.png></li> </ul> </li> <li>MDP에서 state, action 집합이 finite하면 tabular representation 으로 MDP를 표현할 수 있으며 이렇게 표현되는 MDP를 finite state machine 이라 부른다.<ul> <li>이 경우에 <span class=arithmatex>\(p_S, p_R\)</span> 을 알 고 있으면 dynamic programming 테크닉으로 optimal policy 를 구할 수 있다. (Section 2.2 에서 다룬다.)</li> <li>그런데 대부분 이 두 distribution (world model) 을 모르기 때문에 DP 방법으로 최적 정책이 풀리는 경우는 잘 없고 RL 테크닉을 사용하여 good policy 를 구하기 위해 노력한다.</li> </ul> </li> </ul> <h4 id=123-contextual-mdps>1.2.3 Contextual MDPs<a class=headerlink href=#123-contextual-mdps title="Permanent link">&para;</a></h4> <ul> <li>Contextual MDP는 environment의 dynamics와 reward가 hidden static parameter(context라 부름)에 의존되는 MDP 모델을 뜻한다.</li> </ul> <h4 id=124-contextual-bandits>1.2.4 Contextual bandits<a class=headerlink href=#124-contextual-bandits title="Permanent link">&para;</a></h4> <p><img alt="Pasted image 20250214141506.png" src=../attachments/Pasted%20image%2020250214141506.png></p> <h4 id=125-belief-state-mdps>1.2.5 Belief state MDPs<a class=headerlink href=#125-belief-state-mdps title="Permanent link">&para;</a></h4> <ul> <li>state가 확률분포 (belief state or information state)</li> <li> <p>belief state MDP (w/ deterministic dynamics) <img alt="Pasted image 20250212204451.png" src=../attachments/Pasted%20image%2020250212204451.png></p> </li> <li> <p>reward function <img alt="Pasted image 20250212204534.png" src=../attachments/Pasted%20image%2020250212204534.png></p> </li> <li> <p>이 (PO)MDP를 푼다면, 우리는 exploration-exploitation problem의 optimal solution을 얻게 된다.</p> </li> </ul> <h4 id=126-optimization-problems>1.2.6 Optimization problems<a class=headerlink href=#126-optimization-problems title="Permanent link">&para;</a></h4> <p>bandit 문제는 agent가 에이전트가 정보를 수집하기 위해 세계와 상호작용해야 하지만 그렇지 않으면 환경에 영향을 주지 않는 문제의 예입니다. (??? 상호작용하지 않으면 환경에 영향을 주지 않는다?)</p> <ul> <li>즉, agent 의 internal belief state 는 시간에 따라 변화하지만 environment state는 변하지 않는다!</li> <li>이러한 상황은 <span class=arithmatex>\(R\)</span> 이 고정된 모르는 함수일 때, 이 함수를 optimize할 때 발생한다.</li> <li>우리는 <span class=arithmatex>\(R\)</span>의 evaluation을 <code>query the function</code> 이라는 용어를 사용하여 실시한다.</li> <li>agent의 목표는 최대한 적게 querying하면서 <span class=arithmatex>\(R\)</span>의 optimum을 찾는 것이다.</li> </ul> <h5 id=1261-best-arm-identification>1.2.6.1 Best-arm identification<a class=headerlink href=#1261-best-arm-identification title="Permanent link">&para;</a></h5> <ul> <li>표준적인 multi-armed bandit 문제에서는 sum of expected rewards를 최대화하는 게 목표이다.</li> <li>어떤 문제 같은 경우는 <span class=arithmatex>\(T\)</span> trials가 고정되어 있을 때 best arm을 선택하는 문제가 목표로 될 수 있다.<ul> <li>이를 best-arm identification 이라 한다. Formally, optimizing the final reward criterion:</li> </ul> </li> </ul> <div class=arithmatex>\[ V_{\pi,\pi_T} = \mathbb{E}_{p(a_{1:T},r_{1:T}|s_0,\pi)}[R(\hat{a})] \]</div> <p>여기서 <span class=arithmatex>\(\hat{a}=\pi_T (a_{1:T}, r_{1:T})\)</span>는 estimated optimal arm computed by the terminal policy <span class=arithmatex>\(\pi_T\)</span> 이다. - 이러한 문제는 standard bandits에서 사용되는 방법론의 간단한 adapation으로 풀린다고 한다.</p> <h5 id=1262-bayesian-optimization>1.2.6.2 Bayesian optimization<a class=headerlink href=#1262-bayesian-optimization title="Permanent link">&para;</a></h5> <ul> <li>Bayesian optimization 은 gradient-free approach to optimizing expensive blackbox function</li> </ul> <div class=arithmatex>\[ w^* =\operatorname*{argmax}_w R(w) \]</div> <ul> <li><span class=arithmatex>\(R\)</span> 은 unknown function이고 최소한의 action 만을 이용하기를 바람(action 은 <span class=arithmatex>\(R\)</span>의 evaluation에 사용)</li> <li>belief state가 함수 <span class=arithmatex>\(R\)</span>의 분포를 가진다고 가정(<span class=arithmatex>\(s_t\sim p(R|h_t)\)</span>)</li> <li>그리고 그 분포 <span class=arithmatex>\(s_t\)</span>일 때 정해지는 <span class=arithmatex>\(R\)</span>에 대해 <span class=arithmatex>\(w_t=\pi (s_t)\)</span>라는 policy를 정하는듯</li> <li>Bayesian optimization은 <code>infinite arm</code> version of the best-arm identification 문제라고 볼 수 있음(discrete choice of arm <span class=arithmatex>\(a \in \{ 1,\cdots, K\}\)</span>)</li> </ul> <h5 id=1263-active-learning>1.2.6.3 Active learning<a class=headerlink href=#1263-active-learning title="Permanent link">&para;</a></h5> <p>위의 <span class=arithmatex>\(R\)</span>을 최대화하는 <span class=arithmatex>\(w^*\)</span> 를 찾는 것이 BayesOpt 방법이었다면 active learning에서는 서로 다른 <span class=arithmatex>\(w_t\)</span> 를 가지고 evaluating하면서 알려지지 않은 entire function <span class=arithmatex>\(R\)</span> 을 학습하고자 한다.</p> <ul> <li>최적의 전략(optimal strategy)은 미지 함수 <span class=arithmatex>\(R\)</span> 에 대한 belief state를 취하지만(확률분포로 보지만), 이제 최적의 정책은 belief state의 entropy를 줄이기 위해 query point <span class=arithmatex>\(w_t\)</span>를 선택하는 등 다른 방법론으로 <span class=arithmatex>\(R\)</span>을 최대화한다.<ul> <li>R의 확실성?을 높이는 식으로 R을 알려고 하는듯</li> </ul> </li> </ul> <h5 id=1264-stochastic-gradient-descent-sgd>1.2.6.4 Stochastic Gradient Descent (SGD)<a class=headerlink href=#1264-stochastic-gradient-descent-sgd title="Permanent link">&para;</a></h5> <p>SGD를 사용하여 sequential decision making process 를 구성할 수 있다.</p> <ul> <li>action space는 querying the unknown function <span class=arithmatex>\(R\)</span> at locations <span class=arithmatex>\(a_t=w_t\)</span> 로 구성</li> <li>reward <span class=arithmatex>\(r_t = R(w_t)\)</span> </li> <li>BayesOpt와 다르게 gradient <span class=arithmatex>\(g_t = \nabla_w R(w)|_{w_t}\)</span> 를 관찰한다.</li> <li>Environment state 는 true function <span class=arithmatex>\(R\)</span> 에 대한 정보를 가지고 있는데 이 <span class=arithmatex>\(R\)</span>은 agent의 action에 따른 observation(gradient와 reward)을 생성하는데 사용된다.</li> <li>Agent state는 parameter <span class=arithmatex>\(w_t\)</span> 가 사용되고, 추가적으로 Adam 에서 사용되는 first and second moment <span class=arithmatex>\(m_t, v_t\)</span>가 함께 사용되기도 한다. (즉, <span class=arithmatex>\(s_t = (w_t, m_t, v_t)\)</span>)</li> <li>optimizer rule(Adam or SGD or etc.)을 따라 <span class=arithmatex>\(w_t\)</span>를 업데이트하게 되고 그 업데이트 될때의 learning rate <span class=arithmatex>\(\alpha_t\)</span>가 policy에 의해 결정된다. (즉, <span class=arithmatex>\(\alpha = \pi(s_t)\)</span>)</li> </ul> <h4 id=134-dealing-with-partial-observability>1.3.4 Dealing with partial observability<a class=headerlink href=#134-dealing-with-partial-observability title="Permanent link">&para;</a></h4> <h5 id=1341-optimal-solution>1.3.4.1 Optimal solution<a class=headerlink href=#1341-optimal-solution title="Permanent link">&para;</a></h5> <ul> <li>world에 대한 모든 것(<span class=arithmatex>\(p(o|z), p(z'|z,a)\)</span>)을 알 때 Bayesian inference를 통해 답을 원리적으로는 구할 수 있다.(이 경우는 모든 history를 알고 가는 case인 것 같다.)</li> <li>그러나 보통 intractible한 경우가 많아 simpler approximation이 사실상 사용된다.</li> <li><span class=arithmatex>\(p(o_{t+1}|h_t,a_t)\)</span> 에 대해 o를 타겟하여 world latent state없이 학습하려하는 걸 <code>predictive state representation (PSR)</code> 라고 부름</li> <li><code>observable operator models</code>의 아이디어 그리고 <code>successor representations</code>와 관련이 있음</li> </ul> <h5 id=1342-finite-observation-history>1.3.4.2 Finite observation history<a class=headerlink href=#1342-finite-observation-history title="Permanent link">&para;</a></h5> <ul> <li>last <span class=arithmatex>\(k\)</span> observation만 가지고 하는 모델</li> <li><span class=arithmatex>\(o\)</span>가 image인 경우에 frame stacking이라 한다.</li> <li>일반적인 MDP 방법으로 답을 찾아 나간다.</li> </ul> <h5 id=1343-stateful-recurrent-policies>1.3.4.3 Stateful (recurrent) policies<a class=headerlink href=#1343-stateful-recurrent-policies title="Permanent link">&para;</a></h5> <ul> <li>가장 강력한 접근법으로 entire past에 대한 정보 모두를 이용한다.</li> <li>policy를 RNN으로 모델링한다. (이는 R2D2 paper에 사용되었다.)</li> <li>RNN의 hidden state <span class=arithmatex>\(z_t\)</span>가 past observation에 대한 정보를 implicitly summarize한다고 생각한다.</li> <li>explicit notion of belief state or uncertainty에 대한 것이 없기 때문에 <code>information-gathering actions</code>를 수행하려는 계획이 일어나지 않는다.<ul> <li>그러나 meta-learning의 방법론으로 이러한 것을 해결하려 한다.</li> </ul> </li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../Features/Text%20Formatting/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Text Formatting"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Text Formatting </div> </div> </a> <a href=../2024-02-19/ class="md-footer__link md-footer__link--next" aria-label="Next: 2024-02-19"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 2024-02-19 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["navigation.tabs", "navigation.footer"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://unpkg.com/mermaid/dist/mermaid.min.js></script> </body> </html>