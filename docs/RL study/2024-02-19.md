# 2024-02-19

### 1.4 Exploration-exploitation tradeoff
현재 당장 큰 reward를 위해 선택할 것인지 vs 더 넓은 state-action space를 활보함에 따라 구해질 수도 있는 더 큰 reward를 바랄 것인지

#### 1.4.1 Simple heuristics
- Pure exploitation (greedy policy) $a_t = \operatorname*{argmax}_a Q(s,a)$ 
- $\epsilon$-greedy policy $\pi_\epsilon$ parametrized by $\epsilon \in [0,1]$ 
	- $1-\epsilon$의 확률로 $a_t=\operatorname*{argmax}_a \hat{R}_t(s_t,a)$
	- $\epsilon$의 확률로 random action
	- 이 방법은 agent가 all state-action 조합에 대해 계속 exploration을 하게 해 줌.
	- 그러나 이 방법론은 `suboptimal`이다. 왜냐하면 $\epsilon/ |\mathcal{A}|$의 확률로 random action을 고르기 때문이다.
		- 따라서 $\epsilon$을 시간에 따라 0으로 annealing하는 방법을 통해 이러한 suboptimal 현상을 완화할 수 있다.
	- 이 방법의 또 다른 문제점은 `dithering`이라는 것인데, 이는 agent가 계속 해서 무엇을 할지 마음을 바꾼다는 의미이다.
		- 이걸 해결하기 위해서 $\epsilon z$-greedy 방법론([Temporally-Extended ε-Greedy Exploration](https://arxiv.org/abs/2006.01782))이 제시되었는데 방법은 다음과 같다. (FYI: 발음은 easy-greedy)
			- $\epsilon$-greedy와 마찬가지로 $1-\epsilon$의 확률로 exploitation을 한다.
			- $\epsilon$의 확률로는 $n$번 같은 action을 반복한다.
			- $n$은 exponential, uniform, zeta distribution 중에서 sampling된다. (논문에서는 zeta distribution with $\mu=2$가 일관성있게 좋은 결과가 나온다고 실험적으로 보였다.)
			- 이 방법론의 메인 가설 혹은 주장은 $\epsilon$-greedy의 주된 단점이 temporal persistence가 부족하다는 것이다. 이렇게 같은 action으로 쭉 같은 행동을 어느정도 해줘야 local optima를 탈출할 수 있다고 주장한다. 
			- 또한 이 방법들은 야생의 동물 세계에서 동물들이 먹이 채집을 하러 방향을 정할 때 랜덤하게 하나의 방향을 정해서 어느정도 heavy-tailed distribution을 따르는 duration만큼 계속 전진하는 ecological 모델이 optimal 동물 채집 전략이라는 사실이라는 것이 알려졌기에 이에 motivate된 방법론이라 할 수 있다.
	-  $\epsilon$-greedy가 아닌 또 다른 방법으로 `Boltzmann exploration`이라는 것이 있다. 메인 아이디어는 당장의 reward가 높은 action에 높은 확률을 부여하는 것이다.
		- 이 방법론은 reward estimate의 결과에 따라 policy의 변화가 좀더 smoother해지는 것이다.

$$
\pi_\tau(a|s) = \dfrac{\exp (\hat{R}_t(s_t,a)/\tau)}{\sum_{a'}\exp (\hat{R}_t (s_t,a')/\tau)}
$$
$\tau$가 0으로 가면 greedy policy가 가까워지고, $\tau$가 무한대로 가면 uniform에 가까워진다.
(아래 그림은 오타가 있는 듯 함)

![[Pasted image 20250217132345.png]]


- 마지막으로 또다른 접근 법으로는 outcome의 결과(consequences of the outcome)가 uncertain한 (state, action)을 더 explore하도록 하는 방법론이 있다.
	- 이 방법은 `exploration bonus` $R_t^b(s,a)$를 통해 이루어지는데, 이는 state s일 때 action a를 실시한 횟수가 적을 때 큰 값을 부여하도록 bonus가 설계된다.
	- 이 bonus를 원래의 reward에 더 하여 사용하는데, 이렇게 하여 agent가 world에 대해 더 탐색하여 useful information을 얻도록 하는 효과가 있다. 이러한 

#### 1.4.2 Methods based on the belief state MDP


##### 1.4.2.1 Bandit case (Gittins indices)



##### 1.4.2.2 MDP case (Bayes Adaptive MDPs)


#### 1.4.3 Upper confidence bounds (UCBs)


##### 1.4.3.1 Basic idea

$$
a_t = \operatorname*{argmax}_a \tilde{R}_t (s_t,a) 
$$

- UCB는 optimistic estimate가 exploartion을 장려하는 `exploration bonus`의 형태로 볼 수 있다.

##### 1.4.3.2 Bandit case: Frequentist approach
- Frequentist approach에서는 confidence bound를 계산할 때 estimation error의 높은 확률의 상한(upper bound)을 유도하기 위해 `concentratioin inequality` 를 사용한다.

$$
| \hat{R}_t (s,a)-R_t(s,a)| \leq \delta_t (s,a)
$$

- $\hat{R}_t$ : usua estimate of $R$ (often the MLE), $\delta_t$ : properly selected function
- Optimistic reward $\tilde{R}_t (s,a)= \hat{R}_t(s,a) +\delta_t(s,a)$ 로 설정한다.

!!! examples
	context-free Bernoulli bandit, $R(a)\sim \text{Ber}(\mu(a))$ 
	MLE $\hat{R}_t(a) = \hat{\mu}_t (a) = \dfrac{N_t^1(a)}{N_t^0(a)+N_t^1(a)}$
	($N_t^r(a)$ : the number of times (up to step $t-1$)) that action $a$ has been tried and the observed reward $r$.
	`Chernoff-Heffding inequality`를 사용하면 $\delta_t (a) = c / \sqrt{N_t(a)}$ for some constant $c$이고, 결론적으로 $\tilde{R}_t(a)=\hat{\mu}_t(a)+\dfrac{c}{\sqrt{N_t(a)}}$ 

##### 1.4.3.3 Bandit case: Bayesian approach



##### 1.4.3.4 MDP case


#### 1.4.4 Thompson sampling
![[Pasted image 20250214143340.png]]

##### 1.4.4.1 Bandit case

##### 1.4.4.2 MDP case (posterior sampling RL)


