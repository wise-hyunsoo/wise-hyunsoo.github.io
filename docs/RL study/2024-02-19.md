# 2024-02-19

### 1.4 Exploration-exploitation tradeoff



#### 1.4.1 Simple heuristics



#### 1.4.2 Methods based on the belief state MDP


##### 1.4.2.1 Bandit case (Gittins indices)



##### 1.4.2.2 MDP case (Bayes Adaptive MDPs)


#### 1.4.3 Upper confidence bounds (UCBs)


##### 1.4.3.1 Basid idea
$$
a_t = \operatorname*{argmax}_a \tilde{R}_t (s_t,a) 
$$
- UCB는 optimistic estimate가 exploartion을 장려하는 `exploration bonus`의 형태로 볼 수 있다.

##### 1.4.3.2 Bandit case: Frequentist approach
- Frequentist approach에서는 confidence bound를 계산할 때 estimation error의 높은 확률의 상한(upper bound)을 유도하기 위해 `concentratioin inequality` 를 사용한다.

$$
| \hat{R}_t (s,a)-R_t(s,a)| \leq \delta_t (s,a)
$$
- $\hat{R}_t$ : usua estimate of $R$ (often the MLE), $\delta_t$ : properly selected function
- Optimistic reward $\tilde{R}_t (s,a)= \hat{R}_t(s,a) +\delta_t(s,a)$ 로 설정한다.

!!! examples
	context-free Bernoulli bandit, $R(a)\sim \text{Ber}(\mu(a))$ 
	MLE $\hat{R}_t(a) = \hat{\mu}_t (a) = \dfrac{N_t^1(a)}{N_t^0(a)+N_t^1(a)}$
	($N_t^r(a)$ : the number of times (up to step $t-1$)) that action $a$ has been tried and the observed reward $r$.
	`Chernoff-Heffding inequality`를 사용하면 $\delta_t (a) = c / \sqrt{N_t(a)}$ for some constant $c$이고, 결론적으로 $\tilde{R}_t(a)=\hat{\mu}_t(a)+\dfrac{c}{\sqrt{N_t(a)}}$ 

##### 1.4.3.3 Bandit case: Bayesian approach



##### 1.4.3.4 MDP case


#### 1.4.4 Thompson sampling
![[Pasted image 20250214143340.png]]

##### 1.4.4.1 Bandit case

##### 1.4.4.2 MDP case (posterior sampling RL)


