# 2024-02-19

### 1.4 Exploration-exploitation tradeoff
현재 당장 큰 reward를 위해 선택할 것인지 vs 더 넓은 state-action space를 활보함에 따라 구해질 수도 있는 더 큰 reward를 바랄 것인지

#### 1.4.1 Simple heuristics
- Pure exploitation (greedy policy) $a_t = \operatorname*{argmax}_a Q(s,a)$ 
- $\epsilon$-greedy policy $\pi_\epsilon$ parametrized by $\epsilon \in [0,1]$ 
	- $1-\epsilon$의 확률로 $a_t=\operatorname*{argmax}_a \hat{R}_t(s_t,a)$
	- $\epsilon$의 확률로 random action
	- 이 방법은 agent가 all state-action 조합에 대해 계속 exploration을 하게 해 줌.
	- 그러나 이 방법론은 `suboptimal`이다. 왜냐하면 $\epsilon/ |\mathcal{A}|$의 확률로 random action을 고르기 때문이다.
		- 따라서 $\epsilon$을 시간에 따라 0으로 annealing하는 방법을 통해 이러한 suboptimal 현상을 완화할 수 있다.
	- 이 방법의 또 다른 문제점은 `dithering`이라는 것인데, 이는 agent가 계속 해서 무엇을 할지 마음을 바꾼다는 의미이다.


#### 1.4.2 Methods based on the belief state MDP


##### 1.4.2.1 Bandit case (Gittins indices)



##### 1.4.2.2 MDP case (Bayes Adaptive MDPs)


#### 1.4.3 Upper confidence bounds (UCBs)


##### 1.4.3.1 Basic idea

$$
a_t = \operatorname*{argmax}_a \tilde{R}_t (s_t,a) 
$$

- UCB는 optimistic estimate가 exploartion을 장려하는 `exploration bonus`의 형태로 볼 수 있다.

##### 1.4.3.2 Bandit case: Frequentist approach
- Frequentist approach에서는 confidence bound를 계산할 때 estimation error의 높은 확률의 상한(upper bound)을 유도하기 위해 `concentratioin inequality` 를 사용한다.

$$
| \hat{R}_t (s,a)-R_t(s,a)| \leq \delta_t (s,a)
$$

- $\hat{R}_t$ : usua estimate of $R$ (often the MLE), $\delta_t$ : properly selected function
- Optimistic reward $\tilde{R}_t (s,a)= \hat{R}_t(s,a) +\delta_t(s,a)$ 로 설정한다.

!!! examples
	context-free Bernoulli bandit, $R(a)\sim \text{Ber}(\mu(a))$ 
	MLE $\hat{R}_t(a) = \hat{\mu}_t (a) = \dfrac{N_t^1(a)}{N_t^0(a)+N_t^1(a)}$
	($N_t^r(a)$ : the number of times (up to step $t-1$)) that action $a$ has been tried and the observed reward $r$.
	`Chernoff-Heffding inequality`를 사용하면 $\delta_t (a) = c / \sqrt{N_t(a)}$ for some constant $c$이고, 결론적으로 $\tilde{R}_t(a)=\hat{\mu}_t(a)+\dfrac{c}{\sqrt{N_t(a)}}$ 

##### 1.4.3.3 Bandit case: Bayesian approach



##### 1.4.3.4 MDP case


#### 1.4.4 Thompson sampling
![[Pasted image 20250214143340.png]]

##### 1.4.4.1 Bandit case

##### 1.4.4.2 MDP case (posterior sampling RL)


