# 2024-02-19

### 1.4 Exploration-exploitation tradeoff
현재 당장 큰 reward를 위해 선택할 것인지 vs 더 넓은 state-action space를 활보함에 따라 구해질 수도 있는 더 큰 reward를 바랄 것인지

#### 1.4.1 Simple heuristics
- Pure exploitation (greedy policy) $a_t = \operatorname*{argmax}_a Q(s,a)$ 
- $\epsilon$-greedy policy $\pi_\epsilon$ parametrized by $\epsilon \in [0,1]$ 
	- $1-\epsilon$의 확률로 $a_t=\operatorname*{argmax}_a \hat{R}_t(s_t,a)$
	- $\epsilon$의 확률로 random action
	- 이 방법은 agent가 all state-action 조합에 대해 계속 exploration을 하게 해 줌.
	- 그러나 이 방법론은 `suboptimal`이다. 왜냐하면 $\epsilon/ |\mathcal{A}|$의 확률로 random action을 고르기 때문이다.
		- 따라서 $\epsilon$을 시간에 따라 0으로 annealing하는 방법을 통해 이러한 suboptimal 현상을 완화할 수 있다.
	- 이 방법의 또 다른 문제점은 `dithering`이라는 것인데, 이는 agent가 계속 해서 무엇을 할지 마음을 바꾼다는 의미이다.
		- 이걸 해결하기 위해서 $\epsilon z$-greedy 방법론([Temporally-Extended ε-Greedy Exploration](https://arxiv.org/abs/2006.01782))이 제시되었는데 방법은 다음과 같다. (FYI: 발음은 easy-greedy)
			- $\epsilon$-greedy와 마찬가지로 $1-\epsilon$의 확률로 exploitation을 한다.
			- $\epsilon$의 확률로는 $n$번 같은 action을 반복한다.
			- $n$은 exponential, uniform, zeta distribution 중에서 sampling된다. (논문에서는 zeta distribution with $\mu=2$가 일관성있게 좋은 결과가 나온다고 실험적으로 보였다.)
			- 이 방법론의 메인 가설 혹은 주장은 $\epsilon$-greedy의 주된 단점이 temporal persistence가 부족하다는 것이다. 이렇게 같은 action으로 쭉 같은 행동을 어느정도 해줘야 local optima를 탈출할 수 있다고 주장한다. 
			- 또한 이 방법들은 야생의 동물 세계에서 동물들이 먹이 채집을 하러 방향을 정할 때 랜덤하게 하나의 방향을 정해서 어느정도 heavy-tailed distribution을 따르는 duration만큼 계속 전진하는 ecological 모델이 optimal 동물 채집 전략이라는 사실이라는 것이 알려졌기에 이에 motivate된 방법론이라 할 수 있다.
	-  $\epsilon$-greedy가 아닌 또 다른 방법으로 `Boltzmann exploration`이라는 것이 있다. 메인 아이디어는 당장의 reward가 높은 action에 높은 확률을 부여하는 것이다.
		- 이 방법론은 reward estimate의 결과에 따라 policy의 변화가 좀더 smoother해지는 것이다.

$$
\pi_\tau(a|s) = \dfrac{\exp (\hat{R}_t(s_t,a)/\tau)}{\sum_{a'}\exp (\hat{R}_t (s_t,a')/\tau)}
$$

$\tau$가 0으로 가면 greedy policy가 가까워지고, $\tau$가 무한대로 가면 uniform에 가까워진다.
(아래 그림은 오타가 있는 듯 함)

![[Pasted image 20250217132345.png]]


- 마지막으로 또다른 접근 법으로는 outcome의 결과(consequences of the outcome)가 uncertain한 (state, action)을 더 explore하도록 하는 방법론이 있다.
	- 이 방법은 `exploration bonus` $R_t^b(s,a)$를 통해 이루어지는데, 이는 state s일 때 action a를 실시한 횟수가 적을 때 큰 값을 부여하도록 bonus가 설계된다.
	- 이 bonus를 원래의 reward에 더 하여 사용하는데, 이렇게 하여 agent가 world에 대해 더 탐색하여 useful information을 얻도록 하는 효과가 있다. 이러한 reward를 `intrinsic reward` 함수라고 부른다. (Section 5.2.4에서 더 자세히 다루는듯)

#### 1.4.2 Methods based on the belief state MDP
Bayesian approach를 적용하여 exploration-exploitation tradeoff에 대한 optimal solution을 계산할 수 있는데 그에 대해 다루는 section이다.

##### 1.4.2.1 Bandit case (Gittins indices)
모델 파라메터에 대한 belief state, $p(\theta_t|\mathcal{D}_{1:t})$를 recursive하게 계산할 수 있다고 가정하자. 이 상황의 belief state MDP에서 우리는 policy를 어떻게 계산하는가(구하는가)?

context-free bandits with a finite number of arms인 경우에는 optimal policy를 dynamic programming을 통해 구할 수 있다.

- 그 결과는 매 스텝별 table of action probabilities $\pi_t(a_1,\cdots,a_K)$ 으로 표현될 수 있는데, 이를 `Gittins indices`라 부른다.
- 물론 general contextual bandits에 대한 optimal policy는 intractable하다.


![[Pasted image 20250217134705.png]]

##### 1.4.2.2 MDP case (Bayes Adaptive MDPs)
위 방법론을 MDP case에 적용하여 BAMDP(Bayes Adaptive MDP)로 바꿔서 적용할 수 있는데, 이러한 확장적용 방법론이 computationally intractable to solve한 것이 알려져 있어서 다양한 근사 방법으로 BADMP를 푼다고 한다.

#### 1.4.3 Upper confidence bounds (UCBs)
explore-exploit에 대한 optimal solution은 intractable하다. 하지만 직관적인 방법(원리)에 기초한 방법론이 사용될 수 있는데, 그 원리는 `optimism in the face of uncertainty (OFU)` 라는 것이다. 이 원리는 action을 greedy하게 선택하는데, 그 선택의 근거가 optimistic estimate of reward로 인한 선택이 되도록 하는 것이다. 이 방법론이 optimal하다는 것은 `R-max` paper에 증명되어 있는데, 이 증명은 선행 연구였던 `E3` paper의 결과에 기초하여 증명되었다.

이 원리의 가장 일반적인 implementation은 `upper confidence bound (UCB)`라는 개념에 근거하여 이루어지는데, 우리는 bandit case에서 이를 설명하고, 그 다음에 MDP case에 대해 다루겠다.

##### 1.4.3.1 Basic idea
- UCB 방법을 쓰기 위해서는 agent가 optimistic reward function을 사용한다고 가정한다. 즉, 높은 확률로 $\tilde{R}_t(s_t,a) \geq R(s_t,a) \ (\forall a\in \mathcal{A})$ 인 상황에서 다음과 같은 greedy action을 취한다.

$$
a_t = \operatorname*{argmax}_a \tilde{R}_t (s_t,a) 
$$

- UCB는 optimistic estimate가 exploration을 장려하는 `exploration bonus`의 형태로 볼 수 있다.
- 일반적으로 the amount of optimism, $\tilde{R}_t-R$이 시간에 따라 감소하며, 따라서 agent는 exploration을 줄이게 된다.
- 적절하게 설계된 optimistic reward estimate $\tilde{R}$을 통해 UCB 방법론이 많은 variants of bandits에서 near-optimal regret을 가진다는 것이 알려져 있다.
	- optimistic function $\tilde{R}$은 다양한 방법으로 설계될 수 있는데 다음 section들에서 다룬다.

##### 1.4.3.2 Bandit case: Frequentist approach
- Frequentist approach에서는 confidence bound를 계산할 때 estimation error의 높은 확률의 상한(upper bound)을 유도하기 위해 `concentratioin inequality` 를 사용한다.

$$
| \hat{R}_t (s,a)-R_t(s,a)| \leq \delta_t (s,a)
$$

- $\hat{R}_t$ : usua estimate of $R$ (often the MLE), $\delta_t$ : properly selected function
- Optimistic reward $\tilde{R}_t (s,a)= \hat{R}_t(s,a) +\delta_t(s,a)$ 로 설정한다.

!!! examples
	context-free Bernoulli bandit, $R(a)\sim \text{Ber}(\mu(a))$ 
	MLE $\hat{R}_t(a) = \hat{\mu}_t (a) = \dfrac{N_t^1(a)}{N_t^0(a)+N_t^1(a)}$
	($N_t^r(a)$ : the number of times (up to step $t-1$)) that action $a$ has been tried and the observed reward $r$.
	`Chernoff-Heffding inequality`를 사용하면 $\delta_t (a) = c / \sqrt{N_t(a)}$ for some constant $c$이고, 결론적으로 $\tilde{R}_t(a)=\hat{\mu}_t(a)+\dfrac{c}{\sqrt{N_t(a)}}$ 

##### 1.4.3.3 Bandit case: Bayesian approach
Bayesian inference를 통해 upper confidence를 구할 수도 있다. 

![[Pasted image 20250217140814.png]]

##### 1.4.3.4 MDP case
frequentist form의 UCB idea는 MDP case로 확장될 수 있는데 [ACBF02]에서는 UCB를 Q-learning과 혼합하여 다음의 policy를 정의한다.

$$
\pi(a|s) = \mathbb{I} \bigg( a=\operatorname*{argmax}_{a'}\Big[Q(s,a')+ c\sqrt{\log (t)/N_t(s,a') }\Big]\bigg)
$$

이외에도 `UCRL2` 와 같은 더 복잡한 방법도 있다.


#### 1.4.4 Thompson sampling

UCB의 대체 방법으로 `Thompson sampling`이 있는데 이는 `probability matching`이라고도 불린다.
##### 1.4.4.1 Bandit case
![[Pasted image 20250217160148.png]]


![[Pasted image 20250214143340.png]]

위 그림은 Thompson sampling을 이용한 linear regression bandit 예시

Thompson sampling quickly discovers that arm 1 is useless. Initially it pulls arm 2 more, but it adapts to the non-stationary nature of the problem and switches over to arm 0, as shown in Figure 1.6(b). In Figure 1.6(c), we show that the empirical cumulative regret in blue is close to the optimal lower bound in red.

##### 1.4.4.2 MDP case (posterior sampling RL)
 reward와 transition model에 대한 posterior를 유지한 채로 sampling an MDP from this belief state at the start of each episode, solving for the optimal policy corresponding to the sampled MDP, using the resulting policy to collect new data, and then updating the belief state at the end of the episode. This is called `posterior sampling RL`

또다른 방법으로는 policy 혹은 Q-function에 대한 posterior를 유지한 채로(world 모델 대신) 하는 방법이 있는데, 그 중 하나의 implementation으로 `epistemic neural networks`가 있다.

또는 successor features를 활용하기도 하고, `Successor Uncertainties`라는 개념을 사용하여 Q function에 대해 posterior distribution을 유도하기도 한다.
![[Pasted image 20250217161513.png]]
