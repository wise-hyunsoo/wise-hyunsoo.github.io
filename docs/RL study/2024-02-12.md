# 2024-02-12
## 1. Introduction

### 1.1 Sequential decision making
#### 1.1.1 Problem definition
$$
V_\pi (s_0) = \mathbb{E}_{p(a_0,s_1,a_1,\cdots,a_T,s_T|s_0,\pi)} \bigg[ \sum_{t=0}^T R(s_t,a_t)|s_0 \bigg]
$$

		$\begin{align*} p(a_0,s_1,a_1,\cdots,a_T,s_T|s_0,\pi) &= \pi(a_0|s_0)p_{\text{env}}(o_1|a_0)\delta(s_1=U(s_0,a_0,o_1)) \\ &\times \pi(a_1|s_1) p_\text{env}(o_2|a_1,o_1)\delta(s_2=U(s_1,a_1,o_2)) \\ & \times \pi(a_2|s_2)p_\text{env}(o_3|a_{1:2},o_{1:2})\delta(s_3=U(s_2,a_2,o_3)) \end{align*}$

- Remark

$$
p_{S}(s_t|s_{t-1},a_{t-1})=\sum_{\lbrace o_t|s_t=U(s_{t-1},a_{t-1},o_t)\rbrace} p_{\text{env}}(o_t | a_{1:t-1}, o_{1:t-1})
$$

- Maximum expected utility (principle) and Optimal policy

$$
\pi^* = \operatorname*{argmax}_\pi \mathbb{E}_{p_0(s_0)} [ V_\pi(s_0)]
$$    
#### 1.1.2 Universal model
![Pasted image 20250211103219.png](attachments/Pasted%20image%2020250211103219.png)


- hidden state $z_t$ 혹은 world state
	- update는 non-determistic하게 $z_{t+1}=W(z_t,a_t,\epsilon_t^z)$ 
	
- agent는 potentially noisy 혹은 partial observation $o_{t+1}=O(z_{t+1},\epsilon_{t+1}^o)$ 를 보게 됨
	- $o_t$는 *perceptual aliasing*이 있을 수 있음
	
- agent의 . tate $s_t$ 는 environment의 믿음(추측)을 담고 있음. 따라서 belief state 라고 부르기도 함.
	- 내부의 믿음 state가 있기에 perceptual aliasing이 있는 상황에서 외부상황에 대한 추측으로 같은 observation $o$가 관측됐다 하더라도 내부 믿음 상황(agent의 내부 state $s_t$)에 따라 다른 판단(action)을 내릴 수 있게 됨.
	- $s_{t+1}=SU(s_t,a_t,o_{t+1})=U(s_{t+1|t},o_{t+1})=U(P(s_t,a_t),o_{t+1})$ 와 같은 식으로 내부 믿음이 update됨.
		- $SU$: state update function, $P$: prediction function, $U$: update function

- RL 방법론에 따라 agent가 외부 observation을 예측하려는 시도도 함
	- $\hat{o}_{t+1}=D(s_{t+1|t})$ 로 다음 observation을 추측하고 이를 decoder $D$로 모델링하고 이를 훈련에 활용하기도 한다고 함(아마도 Model-based RL에서 하는듯)
	- 참고로 model-based RL에서 모델은 외부환경 혹은 World에 대한 modeling을 말하는 듯
		- 그중 state transition or dynamics model $p_S(s'|s,a)$ 를 학습하는 방법론이 있는 듯

!!! note
	 [On Representation Complexity of Model-based and Model-free Reinforcement Learning](https://arxiv.org/abs/2310.01706) 에서는 이러한 dynamics를 학습하는 듯. 논문에 따르면 `The sample complexity of learning the dynamics is less than the sample complexity of learning the policy.` 이라 한다.

#### 1.1.3 Episodic vs continuing task
`Continuing task`
:   끝이 agent와 environment가 끝없이 상호작용

`Episodic task`
:   언젠가는 끝남. 끝나는 시간이 정해져 있으면 finite horizon problem.


![Pasted image 20250212191708.png](attachments/Pasted%20image%2020250212191708.png)

- reward-to-go $G_t$ 는 random variable. 
- 그 random variable의 $\pi$에 대한 기댓값(정확히는 $s_t$에 대한 조건부 기댓값)으로 value function $V_\pi (s_t)$ 를 정의

- Discount factor $\gamma \in [0,1]$
	- $\gamma \in [0,1)$를 쓰는 이유로 첫번째는 reward가 bounded일 때, value function의 수렴성 보장을 위해 사용, 두번째로는 short-term reward에 가중치를 주려고 사용, 즉 $\gamma =1$인 경우는 현재랑 가까운 reward나 미래에 받는 reward나 그 값이 같으면 똑같다고 생각.
	- $\gamma$가 작을 수록 근시안적(myopic)이 됨.  따라서 $1-\gamma$는 다음 step에서 종료될 기대(혹은 확률)로 해석가능


#### 1.1.4  Regret

- per-step regret at $t$ : $l_t$  
	- oracle policy를 알아야 써먹을 수 있을듯
	
$$
l_t := \mathbb{E}_{s_{1:t}} \big[R(s_t,\pi_*(s_t))-\mathbb{E}_{\pi(a_t|s_t)}[R(s_t,a_t)]\big]
$$

- simple regret at the last step, $l_T$ 
	-  simple regret을 최적화하는 문제를 pure exploration이라 부름

- cumulative regret (or total regret or just the regret) $L_T$ 
	- 이걸로 축적하면서 모델이랑 policy를 학습하면 `earning while learning` 이라 하는듯
	- exploration-exploitation tradeoff를 해결하면서 문제를 풀어야 함
	
$$
L_T := \mathbb{E}\bigg[ \sum_{t=1}^T l_t \bigg]
$$

### 1.2 Canonical examples
#### 1.2.1 Partially observed MDPs (POMDP)
- Figure 1.2의 그림이 POMDP 모델이라 함
- environment dynamic model 을 아래와 같이 stochastic transition function $W$ 으로 

$$
p(z_{t+1}|z_t, a_t) = \mathbb{E}_{\epsilon_t^z}[\mathbb{I}(z_{t+1}=W(z_t,a_t,\epsilon_t^z))]
$$

- stochastic observation function $O$ 은 

$$
p(o_{t+1}|z_{t+1}) = \mathbb{E}_{\epsilon_{t+1}^o} [\mathbb{I}(o_{t+1}=O(z_{t+1},\epsilon_{t+1}^o))]
$$

와 같이 사용하여 `joint world model` $p_{\text{WO}}(z_{t+1},o_{t+1}|z_t,a_t)$ 을 도출

또는 non-Markovian observation distribution $p_{\text{env}} (o_{t+1}|o_{1:t}, a_{1:t})$ 를 아래와 같이 정의
![Pasted image 20250212194031.png](attachments/Pasted%20image%2020250212194031.png)

- 두 개의 transition distribution $p(o|z), p(z'|z,a)$ 가 world model의 정보
	- 두 분포를 안다면(world model 을 안다면) 원리적으로는 optimal policy를 구할 수 있다고 한다.
	- agent의 internal state $s_t$ 혹은 belief state $b_t$를 $s_t=b_t=p(z_t|h_t), \ h_t=(o_{1:t}, a_{1:t-1})$ 로 두면 구할 수 있다고 함
	- belief state는 원리적으로(통계적으로) optimal policy에 대해 충분통계량이라고 함.
		- 데이터로 부터 belief state를 계산하면 그 belief state가 policy의 parameter와 독립적이라는 뜻
		- 그러나 computing belief state is wildly intractible 이라 함.

#### 1.2.2 Markov decision process (MDPs)
- MDP는 special case of POMDP
	-  $z_t=o_t=s_t$ 인 상황을 가정
	- 따라서 `state transition matrix induce by the world model` : $p_S(s_{t+1}|s_t,a_t):=\mathbb{E}_{\epsilon_t^s}[\mathbb{I} (s_{t+1}=W(s_t,a_t,\epsilon_t^s))]$ 로 정의하여 MDP를 기술

- POMDP 에서 사용되던 observation 관점에서 MDP에서는 observation으로 reward를 environment에게서 받는다고 생각한다.
	- expected reward(사실 조건부 기댓값)는 두가지 형태가 있고 다음과 같이 정의 
![Pasted image 20250212195751.png](attachments/Pasted%20image%2020250212195751.png)
- MDP에서 state, action 집합이 finite하면 tabular representation 으로 MDP를 표현할 수 있으며 이렇게 표현되는 MDP를 finite state machine 이라 부른다.
	- 이 경우에 $p_S, p_R$ 을 알 고 있으면 dynamic programming 테크닉으로 optimal policy 를 구할 수 있다. (Section 2.2 에서 다룬다.)
	- 그런데 대부분 이 두 distribution (world model) 을 모르기 때문에 DP 방법으로 최적 정책이 풀리는 경우는 잘 없고 RL 테크닉을 사용하여 good policy 를 구하기 위해 노력한다.

#### 1.2.3 Contextual MDPs
- Contextual MDP는 environment의 dynamics와 reward가 hidden static parameter(context라 부름)에 의존되는 MDP 모델을 뜻한다.

#### 1.2.4 Contextual bandits

![[Pasted image 20250214141506.png]]

#### 1.2.5 Belief state MDPs
- state가 확률분포 (belief state or information state)
- belief state MDP (w/ deterministic dynamics)
![Pasted image 20250212204451.png](attachments/Pasted%20image%2020250212204451.png)

- reward function ![Pasted image 20250212204534.png](attachments/Pasted%20image%2020250212204534.png)

- 이 (PO)MDP를 푼다면, 우리는 exploration-exploitation problem의 optimal solution을 얻게 된다.

#### 1.2.6 Optimization problems
 bandit 문제는 agent가 에이전트가 정보를 수집하기 위해 세계와 상호작용해야 하지만 그렇지 않으면 환경에 영향을 주지 않는 문제의 예입니다. (??? 상호작용하지 않으면 환경에 영향을 주지 않는다?)
 
- 즉, agent 의 internal belief state 는 시간에 따라 변화하지만 environment state는 변하지 않는다!
- 이러한 상황은 $R$ 이 고정된 모르는 함수일 때, 이 함수를 optimize할 때 발생한다.
- 우리는 $R$의 evaluation을 `query the function` 이라는 용어를 사용하여 실시한다.
- agent의 목표는 최대한 적게 querying하면서 $R$의 optimum을 찾는 것이다.

##### 1.2.6.1 Best-arm identification
- 표준적인 multi-armed bandit 문제에서는 sum of expected rewards를 최대화하는 게 목표이다.
- 어떤 문제 같은 경우는 $T$ trials가 고정되어 있을 때 best arm을 선택하는 문제가 목표로 될 수 있다.
	- 이를 best-arm identification 이라 한다. Formally, optimizing the final reward criterion:

$$
V_{\pi,\pi_T} = \mathbb{E}_{p(a_{1:T},r_{1:T}|s_0,\pi)}[R(\hat{a})]
$$

여기서 $\hat{a}=\pi_T (a_{1:T}, r_{1:T})$는 estimated optimal arm computed by the terminal policy $\pi_T$ 이다. 
- 이러한 문제는 standard bandits에서 사용되는 방법론의 간단한 adapation으로 풀린다고 한다.

##### 1.2.6.2 Bayesian optimization
- Bayesian optimization 은 gradient-free approach to optimizing expensive blackbox function

$$
w^* =\operatorname*{argmax}_w R(w)
$$

- $R$ 은 unknown function이고 최소한의 action 만을 이용하기를 바람(action 은 $R$의 evaluation에 사용)
- belief state가 함수 $R$의 분포를 가진다고 가정($s_t\sim p(R|h_t)$)
- 그리고 그 분포 $s_t$일 때 정해지는 $R$에 대해 $w_t=\pi (s_t)$라는 policy를 정하는듯
- Bayesian optimization은 `infinite arm` version of the best-arm identification 문제라고 볼 수 있음(discrete choice of arm $a \in \{ 1,\cdots, K\}$)


##### 1.2.6.3 Active learning
위의 $R$을 최대화하는 $w^*$ 를 찾는 것이 BayesOpt 방법이었다면 active learning에서는 서로 다른 $w_t$ 를 가지고 evaluating하면서 알려지지 않은 entire function $R$ 을 학습하고자 한다.

- 최적의 전략(optimal strategy)은 미지 함수 $R$ 에 대한 belief state를 취하지만(확률분포로 보지만), 이제 최적의 정책은 belief state의 entropy를 줄이기 위해 query point $w_t$를 선택하는 등 다른 방법론으로 $R$을 최대화한다.
	- R의 확실성?을 높이는 식으로 R을 알려고 하는듯

##### 1.2.6.4 Stochastic Gradient Descent (SGD)
SGD를 사용하여 sequential decision making process 를 구성할 수 있다.

- action space는 querying the unknown function $R$ at locations $a_t=w_t$ 로 구성
- reward $r_t = R(w_t)$ 
- BayesOpt와 다르게 gradient $g_t = \nabla_w R(w)|_{w_t}$ 를 관찰한다.
- Environment state 는 true function $R$ 에 대한 정보를 가지고 있는데 이 $R$은 agent의 action에 따른 observation(gradient와 reward)을 생성하는데 사용된다.
- Agent state는 parameter $w_t$ 가 사용되고, 추가적으로 Adam 에서 사용되는 first and second moment $m_t, v_t$가 함께 사용되기도 한다. (즉, $s_t = (w_t, m_t, v_t)$)
- optimizer rule(Adam or SGD or etc.)을 따라 $w_t$를 업데이트하게 되고 그 업데이트 될때의 learning rate $\alpha_t$가 policy에 의해 결정된다. (즉, $\alpha = \pi(s_t)$)

#### 1.3.4 Dealing with partial observability

##### 1.3.4.1 Optimal solution
- world에 대한 모든 것($p(o|z), p(z'|z,a)$)을 알 때 Bayesian inference를 통해 답을 원리적으로는 구할 수 있다.(이 경우는 모든 history를 알고 가는 case인 것 같다.)
- 그러나 보통 intractible한 경우가 많아 simpler approximation이 사실상 사용된다.
- $p(o_{t+1}|h_t,a_t)$ 에 대해 o를 타겟하여 world latent state없이 학습하려하는 걸 `predictive state representation (PSR)` 라고 부름
- `observable operator models`의 아이디어 그리고 `successor representations`와 관련이 있음
##### 1.3.4.2 Finite observation history
- last $k$ observation만 가지고 하는 모델
- $o$가 image인 경우에 frame stacking이라 한다.
- 일반적인 MDP 방법으로 답을 찾아 나간다.

##### 1.3.4.3 Stateful (recurrent) policies
- 가장 강력한 접근법으로 entire past에 대한 정보 모두를 이용한다.
- policy를 RNN으로 모델링한다. (이는 R2D2 paper에 사용되었다.)
- RNN의 hidden state $z_t$가 past observation에 대한 정보를 implicitly summarize한다고 생각한다.
- explicit notion of belief state or uncertainty에 대한 것이 없기 때문에 `information-gathering actions`를 수행하려는 계획이 일어나지 않는다.
	- 그러나 meta-learning의 방법론으로 이러한 것을 해결하려 한다.
	
