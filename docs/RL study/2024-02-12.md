# 2024-02-12
## 1. Introduction

### 1.1 Sequential decision making
#### 1.1.1 Problem definition
![Pasted image 20250211093756.png](attachments/Pasted%20image%2020250211093756.png)
![Pasted image 20250211093819.png](attachments/Pasted%20image%2020250211093819.png)

- Remark

$$
p_{S}(s_t|s_{t-1},a_{t-1})=\sum_{\lbrace o_t|s_t=U(s_{t-1},a_{t-1},o_t)\rbrace} p_{\text{env}}(o_t | a_{1:t-1}, o_{1:t-1})
$$

- Maximum expected utility (principle) and Optimal policy

![Pasted image 20250211093735.png](attachments/Pasted%20image%2020250211093735.png)
#### 1.1.2 Universal model
![Pasted image 20250211103219.png](attachments/Pasted%20image%2020250211103219.png)


- hidden state $z_t$ 혹은 world state
	- update는 non-determistic하게 $z_{t+1}=W(z_t,a_t,\epsilon_t^z)$ 
	
- agent는 potentially noisy 혹은 partial observation $o_{t+1}=O(z_{t+1},\epsilon_{t+1}^o)$ 를 보게 됨
	- $o_t$는 *perceptual aliasing*이 있을 수 있음
	
- agent의 state $s_t$ 는 environment의 믿음(추측)을 담고 있음. 따라서 belief state 라고 부르기도 함.
	- 내부의 믿음 state가 있기에 perceptual aliasing이 있는 상황에서 외부상황에 대한 추측으로 같은 observation $o$가 관측됐다 하더라도 내부 믿음 상황(agent의 내부 state $s_t$)에 따라 다른 판단(action)을 내릴 수 있게 됨.
	- $s_{t+1}=SU(s_t,a_t,o_{t+1})=U(s_{t+1|t},o_{t+1})=U(P(s_t,a_t),o_{t+1})$ 와 같은 식으로 내부 믿음이 update됨.
		- $SU$: state update function, $P$: prediction function, $U$: update function

- RL 방법론에 따라 agent가 외부 observation을 예측하려는 시도도 함
	- $\hat{o}_{t+1}=D(s_{t+1|t})$ 로 다음 observation을 추측하고 이를 decoder $D$로 모델링하고 이를 훈련에 활용하기도 한다고 함(아마도 Model-based RL에서 하는듯)
	- 참고로 model-based RL에서 모델은 외부환경 혹은 World에 대한 modeling을 말하는 듯
		- 그중 state transition or dynamics model $p_S(s'|s,a)$ 를 학습하는 방법론이 있는 듯

!!! note
	 [On Representation Complexity of Model-based and Model-free Reinforcement Learning](https://arxiv.org/abs/2310.01706) 에서는 이러한 dynamics를 학습하는 듯. 논문에 따르면 `The sample complexity of learning the dynamics is less than the sample complexity of learning the policy.` 이라 한다.

#### 1.1.3 Episodic vs continuing task
`Continuing task`
:   끝이 agent와 environment가 끝없이 상호작용

`Episodic task`
:   언젠가는 끝남. 끝나는 시간이 정해져 있으면 finite horizon problem.


![[Pasted image 20250212191708.png]]

- reward-to-go $G_t$ 는 random variable. 
- 그 random variable의 $\pi$에 대한 기댓값(정확히는 $s_t$에 대한 조건부 기댓값)으로 value function $V_\pi (s_t)$ 를 정의

- Discount factor $\gamma \in [0,1]$
	- $\gamma \in [0,1)$를 쓰는 이유로 첫번째는 reward가 bounded일 때, value function의 수렴성 보장을 위해 사용, 두번째로는 short-term reward에 가중치를 주려고 사용, 즉 $\gamma =1$인 경우는 현재랑 가까운 reward나 미래에 받는 reward나 그 값이 같으면 똑같다고 생각.
	- $\gamma$가 작을 수록 근시안적(myopic)이 됨.  따라서 $1-\gamma$는 다음 step에서 종료될 기대(혹은 확률)로 해석가능


#### 1.1.4  Regret

- per-step regret at $t$ : $l_t$  
	- oracle policy를 알아야 써먹을 수 있을듯
![[Pasted image 20250212192413.png]]

- simple regret at the last step, $l_T$ 
	-  simple regret을 최적화하는 문제를 pure exploration이라 부름

- cumulative regret (or total regret or just the regret) $L_T$ 
	- 이걸로 축적하면서 모델이랑 policy를 학습하면 `earning while learning` 이라 하는듯
	- exploration-exploitation tradeoff를 해결하면서 문제를 풀어야 함
![[Pasted image 20250212192551.png]]

### 1.2 Canonical examples
#### 1.2.1 Partially observed MDPs (POMDP)
- environment dynamic model 을 아래와 같이 stochastic transition function $W$ 으로 
![[Pasted image 20250212193513.png]]

- stochastic observation function $O$ 은 
![[Pasted image 20250212193546.png]]

와 같이 사용하여 `joint world model` $p_{\text{WO}}(z_{t+1},o_{t+1}|z_t,a_t)$ 을 도출

또는 non-Markovian observation distribution $p_{\text{env}} (o_{t+1}|o_{1:t}, a_{1:t})$ 를 아래와 같이 정의
![[Pasted image 20250212194031.png]]
