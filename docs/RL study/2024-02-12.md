# 2024-02-12
## 1. Introduction

### 1.1 Sequential decision making
#### 1.1.1 Problem definition
![Pasted image 20250211093756.png](attachments/Pasted%20image%2020250211093756.png)
![Pasted image 20250211093819.png](attachments/Pasted%20image%2020250211093819.png)

- Remark

$$
p_{S}(s_t|s_{t-1},a_{t-1})=\sum_{\lbrace o_t|s_t=U(s_{t-1},a_{t-1},o_t)\rbrace} p_{\text{env}}(o_t | a_{1:t-1}, o_{1:t-1})
$$

- Maximum expected utility (principle) and Optimal policy

![Pasted image 20250211093735.png](attachments/Pasted%20image%2020250211093735.png)
#### 1.1.2 Universal model
![Pasted image 20250211103219.png](attachments/Pasted%20image%2020250211103219.png)


- hidden state $z_t$ 혹은 world state
	- update는 non-determistic하게 $z_{t+1}=W(z_t,a_t,\epsilon_t^z)$ 
	
- agent는 potentially noisy 혹은 partial observation $o_{t+1}=O(z_{t+1},\epsilon_{t+1}^o)$ 를 보게 됨
	- $o_t$는 *perceptual aliasing*이 있을 수 있음
	
- agent의 state $s_t$ 는 environment의 믿음(추측)을 담고 있음. 따라서 belief state 라고 부르기도 함.
	- 내부의 믿음 state가 있기에 perceptual aliasing이 있는 상황에서 외부상황에 대한 추측으로 같은 observation $o$가 관측됐다 하더라도 내부 믿음 상황(agent의 내부 state $s_t$)에 따라 다른 판단(action)을 내릴 수 있게 됨.
	- $s_{t+1}=SU(s_t,a_t,o_{t+1})=U(s_{t+1|t},o_{t+1})=U(P(s_t,a_t),o_{t+1})$ 와 같은 식으로 내부 믿음이 update됨.
		- $SU$: state update function, $P$: prediction function, $U$: update function

- RL 방법론에 따라 agent가 외부 observation을 예측하려는 시도도 함
	- $\hat{o}_{t+1}=D(s_{t+1|t})$ 로 다음 observation을 추측하고 이를 decoder $D$로 모델링하고 이를 훈련에 활용하기도 한다고 함(아마도 Model-based RL에서 하는듯)
	- 참고로 model-based RL에서 모델은 외부환경 혹은 World에 대한 modeling을 말하는 듯
		- 그중 state transition or dynamics model $p_S(s'|s,a)$ 를 학습하는 방법론이 있는 듯

!!! note
	 [On Representation Complexity of Model-based and Model-free Reinforcement Learning](https://arxiv.org/abs/2310.01706) 에서는 이러한 dynamics를 학습하는 듯. 논문에 따르면 `The sample complexity of learning the dynamics is less than the sample complexity of learning the policy.` 이라 한다.

#### 1.1.3 Episodic vs continuing task
`Continuing task`
:   끝이 agent와 environment가 끝없이 상호작용

`Episodic task`
:   언젠가는 끝남. 끝나는 시간이 정해져 있으면 finite horizon problem.


![[Pasted image 20250212191708.png]]

- reward-to-go $G_t$ 는 random variable. 
- 그 random variable의 $\pi$에 대한 기댓값(정확히는 $s_t$에 대한 조건부 기댓값)으로 value function $V_\pi (s_t)$ 를 정의

- Discount factor $\gamma \in [0,1]$
	- $\gamma \in [0,1)$를 쓰는 이유로 첫번째는 reward가 bounded일 때, value function의 수렴성 보장을 위해 사용, 두번째로는 short-term reward에 가중치를 주려고 사용, 즉 $\gamma =1$인 경우는 현재랑 가까운 reward나 미래에 받는 reward나 그 값이 같으면 똑같다고 생각.
	- $\gamma$가 작을 수록 근시안적(myopic)이 됨.  따라서 $1-\gamma$는 다음 step에서 종료될 기대(혹은 확률)로 해석가능


#### 1.1.4  Regret

- per-step regret at $t$ : $l_t$  
	- oracle policy를 알아야 써먹을 수 있을듯
![[Pasted image 20250212192413.png]]

- simple regret at the last step, $l_T$ 
	-  simple regret을 최적화하는 문제를 pure exploration이라 부름

- cumulative regret (or total regret or just the regret) $L_T$ 
	- 이걸로 축적하면서 모델이랑 policy를 학습하면 `earning while learning` 이라 하는듯
	- exploration-exploitation tradeoff를 해결하면서 문제를 풀어야 함
![[Pasted image 20250212192551.png]]

### 1.2 Canonical examples
#### 1.2.1 Partially observed MDPs (POMDP)
- Figure 1.2의 그림이 POMDP 모델이라 함
- environment dynamic model 을 아래와 같이 stochastic transition function $W$ 으로 
![[Pasted image 20250212193513.png]]

- stochastic observation function $O$ 은 
![[Pasted image 20250212193546.png]]

와 같이 사용하여 `joint world model` $p_{\text{WO}}(z_{t+1},o_{t+1}|z_t,a_t)$ 을 도출

또는 non-Markovian observation distribution $p_{\text{env}} (o_{t+1}|o_{1:t}, a_{1:t})$ 를 아래와 같이 정의
![[Pasted image 20250212194031.png]]

- 두 개의 transition distribution $p(o|z), p(z'|z,a)$ 가 world model의 정보
	- 두 분포를 안다면(world model 을 안다면) 원리적으로는 optimal policy를 구할 수 있다고 한다.
	- agent의 internal state $s_t$ 혹은 belief state $b_t$를 $s_t=b_t=p(z_t|h_t), \ h_t=(o_{1:t}, a_{1:t-1})$ 로 두면 구할 수 있다고 함
	- belief state는 원리적으로(통계적으로) optimal policy에 대해 충분통계량이라고 함.
		- 데이터로 부터 belief state를 계산하면 그 belief state가 policy의 parameter와 독립적이라는 뜻
		- 그러나 computing belief state is wildly intractible 이라 함.

#### 1.2.2 Markov decision process (MDPs)
- MDP는 special case of POMDP
	-  $z_t=o_t=s_t$ 인 상황을 가정
	- 따라서 `state transition matrix induce by the world model` : $p_S(s_{t+1}|s_t,a_t):=\mathbb{E}_{\epsilon_t^s}[\mathbb{I} (s_{t+1}=W(s_t,a_t,\epsilon_t^s))]$ 로 정의하여 MDP를 기술

- POMDP 에서 사용되던 observation 관점에서 MDP에서는 observation으로 reward를 environment에게서 받는다고 생각한다.
	- expected reward(사실 조건부 기댓값)는 두가지 형태가 있고 다음과 같이 정의 
![[Pasted image 20250212195751.png]]
- MDP에서 state, action 집합이 finite하면 tabular representation 으로 MDP를 표현할 수 있으며 이렇게 표현되는 MDP를 finite state machine 이라 부른다.
	- 이 경우에 $p_S, p_R$ 을 알 고 있으면 dynamic programming 테크닉으로 optimal policy 를 구할 수 있다. (Section 2.2 에서 다룬다.)
	- 그런데 대부분 이 두 distribution (world model) 을 모르기 때문에 DP 방법으로 최적 정책이 풀리는 경우는 잘 없고 RL 테크닉을 사용하여 good policy 를 구하기 위해 노력한다.

#### 1.2.3 Contextual MDPs
- Contextual MDP는 environment의 dynamics와 reward가 hidden static parameter(context라 부름)에 의존되는 MDP 모델을 뜻한다.

#### 1.2.4 Contextual bandits